
import { QuestionCategory } from '../types';

const dataEngineeringCategory: QuestionCategory = {
    id: 'data_engineering',
    title: 'Data Engineering & Architecture',
    icon: 'fa-sitemap',
    description: 'Design and implementation of data pipelines, storage systems, and large-scale data processing architectures.',
    questions: [
        {
            id: 'de-1',
            question: 'Compare Data Warehouse vs. Data Lake vs. Data Lakehouse. How would you design a unified architecture?',
            concepts: '**Data Warehouse**: Stores structured, filtered data for a specific purpose (BI, analytics). Schema-on-write.\n**Data Lake**: Stores raw, unstructured/structured data in its native format. Schema-on-read.\n**Data Lakehouse**: A hybrid architecture that combines the low-cost, flexible storage of a data lake with the ACID transactions and data management features of a data warehouse.',
            answer: 'This represents the evolution of data architecture:\n- **Data Warehouse (DWH)**: Highly structured, optimized for fast SQL queries and business intelligence. Data must be cleaned and modeled *before* being loaded (schema-on-write). Best for traditional reporting.\n- **Data Lake**: A vast storage repository for all types of data (structured, semi-structured, raw logs, images). It\'s flexible and cheap but can become a "data swamp" without proper governance.\n- **Data Lakehouse**: The modern paradigm. It uses a data lake for storage (like S3 or ADLS) but adds a transactional management layer on top, most commonly **Delta Lake**. This provides DWH features like ACID transactions, schema enforcement, and time travel directly on the data lake files.\n\n**Unified Architecture (Medallion Architecture)**:\n1.  **Bronze Layer**: Ingest all raw source data into the data lake (Delta format) with minimal changes.\n2.  **Silver Layer**: Clean, conform, and join the bronze data into more validated, enriched tables. This is where data quality rules are enforced.\n3.  **Gold Layer**: Aggregate the silver data into business-level, project-specific tables, optimized for analytics and reporting. This layer is what business analysts and data scientists consume.',
            example: 'A company ingests raw JSON clickstream data into the **Bronze** layer. A daily Spark job cleans and joins this data with user dimension tables to create a sessionized **Silver** table. Finally, another job aggregates the silver table into a **Gold** table summarizing daily active users and top product views for Power BI dashboards.'
        },
        {
            id: 'de-2',
            question: 'You have millions of JSON files arriving daily in Azure Data Lake. How would you design an incremental ingestion pipeline using ADF + Databricks with schema evolution handling?',
            concepts: '**Incremental Loading**: Processing only new or changed data since the last run.\n**Schema Evolution**: The ability of a pipeline to gracefully handle changes in the source data schema (new columns, data type changes).\n**ADF + Databrabs + Delta Lake**: A common Azure data engineering stack.',
            answer: 'This is a classic data engineering design problem. The best practice is to use **Databricks Auto Loader** with **Delta Lake**.\n\n**Pipeline Design**:\n1.  **ADF for Orchestration**: An Azure Data Factory pipeline is triggered on a schedule (e.g., every 15 minutes).\n2.  **ADF Trigger**: The ADF pipeline\'s only job is to execute a Databricks notebook.\n3.  **Databricks for Processing (Auto Loader)**: The notebook uses Spark Structured Streaming with Auto Loader to read the JSON files from the landing zone in ADLS. Auto Loader automatically and efficiently keeps track of which files have been processed, making incremental ingestion simple.\n4.  **Schema Evolution Handling**: Auto Loader is configured to handle schema evolution. We can set it to infer the schema and use Delta Lake\'s schema merge capabilities. When a new column appears in a source JSON file, the read operation won\'t fail. Instead, the `mergeSchema` option on the Delta Lake write operation will automatically and safely add the new column to the target Delta table without rewriting old data.\n5.  **Write to Delta Lake**: The streaming query writes the data into a Delta Lake table (e.g., in the Bronze layer). Delta Lake provides the transactional guarantees and performance needed.',
            example: '---CODE_START---python\n# In the Databricks notebook\n(spark.readStream\n  .format("cloudFiles")\n  .option("cloudFiles.format", "json")\n  .option("cloudFiles.schemaLocation", "/path/to/schema/logs") # For schema inference and tracking\n  .load("/path/to/landing/zone/")\n  .writeStream\n  .format("delta")\n  .option("checkpointLocation", "/path/to/checkpoints")\n  .option("mergeSchema", "true") # Key for schema evolution\n  .start("/path/to/bronze/table"))\n---CODE_END---'
        },
        {
            id: 'de-3',
            question: 'Explain how Delta Lake performs time travel and how it can be used for auditing and rollback.',
            concepts: '**Delta Lake**: A transactional storage layer that runs on top of a data lake.\n**Transaction Log**: The core of Delta Lake. A JSON log that provides an ordered record of every transaction performed on a table.\n**Time Travel**: The ability to query a snapshot of a table as it existed at a specific point in time or version.',
            answer: 'Delta Lake\'s power comes from its **transaction log** (the `_delta_log` directory). Every operation (like an `INSERT`, `UPDATE`, `DELETE`, or `MERGE`) that modifies a Delta table is recorded as a new, atomically committed JSON file in this log. A Delta table is not just the Parquet data files; it\'s the Parquet files **plus** this log.\n\n**How Time Travel Works**:\nWhen you query a Delta table, Spark first consults the transaction log to determine which Parquet files are part of the latest version of the table. To time travel, you simply tell Spark to use an older version of the log.\n- `df = spark.read.format("delta").option("versionAsOf", 5).load(...)`\n- `df = spark.read.format("delta").option("timestampAsOf", "2023-11-20").load(...)`\nSpark will read the log up to that version/timestamp and only use the data files that were valid at that point.\n\n**Use Cases**:\n- **Auditing**: You can query the state of your data at a specific time to see what the values were before a change was made, which is crucial for compliance and debugging.\n- **Rollback**: If a pipeline run introduces bad data, you can easily roll back the entire table to the version before the error occurred with a simple command: `RESTORE TABLE my_table TO VERSION AS OF 123`.',
            example: 'A junior developer runs a `DELETE` query without a `WHERE` clause, accidentally wiping out a production table. With Delta Lake, you can find the version number of the table right before the mistake using `DESCRIBE HISTORY`, and then run a `RESTORE` command to instantly recover the lost data, turning a disaster into a minor inconvenience.'
        },
        {
            id: 'de-4',
            question: 'What are Z-Ordering and Bloom Filters in Delta Lake? How do they improve query performance?',
            concepts: '**Data Skipping**: A performance optimization technique where queries can skip reading files that are known not to contain relevant data.\n**Z-Ordering**: A technique to co-locate related information in the same set of files.\n**Bloom Filter**: A space-efficient probabilistic data structure used to test whether an element is a member of a set.',
            // FIX: Escaped single quotes in string literals to prevent parsing errors.
            answer: 'Both are optimization techniques that enhance Delta Lake\'s **data skipping** capabilities, allowing queries to run much faster by reading less data.\n- **Z-Ordering**: This is a form of multi-dimensional clustering. When you `OPTIMIZE` a Delta table with a `ZORDER BY` clause on several columns, Delta physically rewrites the data files so that data points with similar values across those columns are placed in the same file. This is extremely effective for queries that filter on the Z-ordered columns, as Spark can skip a huge number of files that don\'t fall within the filter range.\n- **Bloom Filters**: A bloom filter is a small index file created for a specific column. It can tell you with 100% certainty if a value **is not** in the file, and with a high probability if a value **might be** in the file. When you query a column that has a bloom filter index, Spark first checks the index. If the index says the value isn\'t there, Spark can skip reading the entire data file. This is most useful for columns with high cardinality and for point-lookups (e.g., `WHERE user_id = \\\'some_specific_id\\\'`).',
            example: 'Imagine a sales table with 10,000 files, partitioned by date. If you frequently query for `product_id` and `region`, you would **Z-Order** by `(product_id, region)`. A query like `WHERE product_id = 123 AND region = \\\'West\\\'` could then potentially skip 9,900 files and only read the few that contain data for that combination. If you often run `WHERE customer_name = \\\'John Doe\\\'`, creating a **Bloom Filter** on the `customer_name` column would allow Spark to avoid reading files that don\\\'t contain that name.'
        },
        {
            id: 'de-5',
            question: 'What are best practices for ADF pipeline orchestration when dealing with dependent activities and failure handling?',
            concepts: '**Orchestration**: The process of coordinating and managing complex workflows.\n**Control Flow**: Using activities like loops, conditions, and dependencies to manage the pipeline\'s execution path.\n**Failure Handling**: Designing pipelines that can gracefully handle and report errors.',
            answer: 'Robust ADF pipelines require careful design of control flow and error handling.\n\n**Best Practices**:\n1.  **Use Dependency Chains**: Connect activities using success (`on success`), failure (`on failure`), or completion (`on completion`) dependencies. This allows you to create branching logic. For example, run a cleanup script only if the main activity fails.\n2.  **Parameterize Everything**: Don\'t hardcode server names, file paths, or table names. Use parameters to make your pipelines reusable and configurable across different environments (dev, test, prod).\n3.  **Group Activities**: Use the `Execute Pipeline` activity to call child pipelines. This promotes modularity and reusability. For example, have a single, robust "ingestion" pipeline that is called by multiple parent pipelines.\n4.  **Implement a Centralized Logging and Alerting Strategy**: Create a dedicated "failure handling" path. If an activity fails, use its `on failure` dependency to trigger a stored procedure or a Logic App that logs the error details to a central table and sends an email or Teams notification to the support team.',
            example: 'A common pattern: A `Copy Data` activity is the main task. Its `on success` dependency leads to the next step in the pipeline. Its `on failure` dependency leads to a `Stored Procedure` activity that writes the `pipeline().RunId` and error message to an audit table, and then sends an alert.'
        },
        {
            id: 'de-6',
            question: 'Compare Synapse Pipelines vs. Azure Data Factory. When would you use each in an enterprise setup?',
            concepts: '**Azure Synapse Analytics**: A unified analytics platform that combines data warehousing, big data processing (Spark), and data integration (pipelines) in a single workspace.\n**Azure Data Factory (ADF)**: A standalone, cloud-based data integration and ETL service.',
            answer: 'ADF and Synapse Pipelines share the same underlying engine and have a very similar UI, but they are designed for different use cases.\n\n- **Azure Data Factory (ADF)**:\n  - **Role**: A standalone, enterprise-wide data integration and orchestration tool.\n  - **Use When**: You need to connect to a vast array of sources (over 90+ connectors), including on-premises data and other clouds. It\'s ideal as the central orchestration tool that can trigger and manage workflows across your entire data estate, including Databricks, Azure Functions, etc.\n\n- **Synapse Pipelines**:\n  - **Role**: The data integration component *within* the Azure Synapse Analytics workspace.\n  - **Use When**: Your analytics ecosystem is primarily centered within Synapse. Its key advantage is the deep and seamless integration with other Synapse components like the Synapse SQL Pools (Dedicated and Serverless) and Synapse Spark Pools. It simplifies development when your source, transformation, and destination are all inside Synapse.\n\n**Decision Criteria**: If your organization has a diverse data landscape and uses multiple best-of-breed tools like Databricks and Snowflake, **ADF** is the better choice for central orchestration. If your entire analytics platform is being built within the unified Synapse ecosystem, then using **Synapse Pipelines** offers a more streamlined, integrated experience.',
            example: 'A company using a Synapse Dedicated SQL Pool as its data warehouse would use **Synapse Pipelines** to load data from Azure Data Lake into the warehouse. A different company that uses Databricks for ETL and Snowflake for its DWH would use **ADF** to orchestrate the entire process, triggering the Databricks notebooks and the Snowflake load procedures in the correct sequence.'
        },
        {
            id: 'de-7',
            question: 'Design a real-time data ingestion pipeline on Azure. How would you handle backpressure and exactly-once delivery?',
            concepts: '**Real-time Processing**: Processing data as it is generated, with very low latency.\n**Backpressure**: A mechanism to handle situations where a data stream is producing data faster than the downstream components can consume it.\n**Exactly-Once Semantics**: A guarantee that each message is processed exactly once, even in the event of failures.',
            answer: 'A robust, scalable real-time pipeline on Azure typically uses the following components:\n\n**Architecture**: **Event Hubs -> Azure Databricks (Structured Streaming) -> Delta Lake -> Power BI**\n\n1.  **Ingestion (Azure Event Hubs)**: Event Hubs is a highly scalable event streaming platform. It acts as the front door, ingesting millions of events per second and providing a durable buffer.\n2.  **Processing (Databricks Structured Streaming)**: A Databricks job using Spark Structured Streaming reads from the Event Hubs stream. Structured Streaming processes the data in small micro-batches, providing low latency while maintaining the fault-tolerance benefits of batch processing.\n3.  **Storage (Delta Lake)**: The processed data is written to a Delta Lake table. Delta Lake provides ACID transactions, which is critical for reliability.\n4.  **Serving (Power BI)**: A Power BI report in DirectQuery mode connects to the Delta table (via the Databricks SQL endpoint) to provide a live, real-time dashboard.\n\n**Handling Key Challenges**:\n- **Backpressure**: This architecture handles backpressure naturally. If Databricks starts to slow down, Structured Streaming will automatically reduce the rate at which it pulls data from Event Hubs, because Event Hubs retains the data in its buffer. This prevents the processing cluster from being overwhelmed.\n- **Exactly-Once Delivery**: This is achieved by using a **transactional sink** and a **replayable source**. Event Hubs is a replayable source (you can re-read the stream from an earlier offset). Delta Lake is a transactional sink. Structured Streaming uses **checkpointing** to save the current offset of the Event Hubs stream to persistent storage. If a job fails, it restarts from the last saved checkpoint, re-reads the stream, and the transactional nature of Delta Lake ensures that the data is written exactly once without duplicates.',
            example: 'This architecture is ideal for an IoT sensor monitoring system, where sensor data flows through Event Hubs, is aggregated in real-time by a Databricks streaming job, and is visualized on a live Power BI dashboard for operators.'
        },
        {
            id: 'de-8',
            question: 'A Databricks notebook takes 3 hours to run daily. How would you profile and optimize it step-by-step?',
            concepts: '**Profiling**: The process of analyzing a program to determine its performance characteristics and identify bottlenecks.\n**Spark UI**: A web interface for monitoring and debugging Spark applications.\n**Optimization Techniques**: Strategies for improving the performance of Spark code.',
            answer: 'Optimizing a long-running job requires a systematic, data-driven approach, not guesswork.\n\n**Step-by-Step Plan**:\n1.  **Analyze with the Spark UI**: This is the most critical first step. The Spark UI provides a detailed breakdown of jobs, stages, and tasks. I would look for:\n    - **Long-running Stages**: Identify which parts of the code are taking the most time.\n    - **Data Skew**: Look at the task duration summary. If a few tasks are taking much longer than the median, it indicates data skew.\n    - **Spill**: Check for memory or disk spill, which means the data doesn\'t fit in memory and Spark is writing intermediate data to disk, which is very slow.\n    - **Shuffle Read/Write**: Look for stages with large amounts of data being shuffled across the network.\n\n2.  **Common Bottlenecks & Solutions**:\n    - **Inefficient Joins**: If I see a large shuffle from a join, I would check if one of the tables is small enough for a **broadcast join** to eliminate the shuffle.\n    - **Data Skew**: If I identify skew on a join or aggregation key, I would implement **salting** to redistribute the data more evenly.\n    - **Inefficient File Reads**: If the job is reading a huge, unpartitioned table, I would add partitioning to the source table (e.g., by date) so the job only reads the required data.\n    - **Complex UDFs**: If profiling shows a Python UDF is slow, I would try to rewrite that logic using Spark\'s built-in SQL functions for better performance.\n\n3.  **Tune Cluster Configuration**: After code optimization, I would analyze the cluster usage. Are the CPUs maxed out while memory is low? I might switch to a CPU-optimized cluster. Is there a lot of memory spill? I would use memory-optimized instances or increase the number of workers.',
            example: 'In a recent project, I used the Spark UI to discover that 90% of a 2-hour job was spent in a single stage. The task summary showed extreme skew. The fix was to implement salting on the problematic join key, which redistributed the workload evenly and reduced the job runtime to 25 minutes.'
        },
        {
            id: 'de-9',
            question: 'Data is stored in multiple schemas or even databases. What approach would you use to merge or unify this data?',
            concepts: '**ETL/ELT**, **Data Virtualization**, **Unified Data Model**, **Master Data Management (MDM)**. Integration strategies.',
            answer: 'There are a few approaches depending on the latency requirements:\n1.  **ETL/ELT (Physical Integration)**: Build pipelines (using ADF, Airflow) to extract data from all sources, transform it into a common schema (conformed dimensions), and load it into a central Data Warehouse or Data Lake. This is best for heavy analytics.\n2.  **Data Virtualization (Logical Integration)**: Use a tool (like Denodo or Synapse Serverless) that queries the sources in real-time and joins them on the fly. No data is moved. Best for ad-hoc queries or when data freshness is critical.\n3.  **Master Data Management (MDM)**: If the issue is inconsistent entity definitions (e.g., "Customer" means something different in Sales vs. Marketing), implementing an MDM solution is necessary to create a "Golden Record" or single source of truth before merging.',
            example: 'In a merger situation, we had to combine sales data from Salesforce (Cloud) and an on-premise Oracle database. I designed an ELT process using Azure Data Factory to dump both datasets into a Data Lake. Then, I used Databricks to map the distinct schemas to a new, unified "Global Sales" schema (Silver layer), which became the single source of truth for the company\'s reporting.'
        }
    ]
};

export default dataEngineeringCategory;
