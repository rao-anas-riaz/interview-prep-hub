import { QuestionCategory } from '../types';

const mlAlgorithmsScratchCategory: QuestionCategory = {
    id: 'ml_algorithms_scratch',
    title: 'ML Algorithms from Scratch',
    icon: 'fa-code-branch',
    description: 'Practical, hands-on coding exercises to implement common ML algorithms and techniques.',
    questions: [
        {
            id: 'pychal-4',
            question: 'Implement your own train-test split without sklearn.',
            concepts: '**Core Concepts**: Model Validation, Overfitting, Shuffling, Reproducibility.\n**Explanation**: Splitting your data into a training set (for the model to learn from) and a test set (for evaluation) is the most fundamental step in model validation. It allows you to assess how well your model generalizes to new, unseen data. Without it, you risk overfitting, where the model simply memorizes the training data and performs poorly in the real world.',
            answer: '---CODE_START---python\nimport numpy as np\n\ndef train_test_split_manual(X, y, test_size=0.2, random_state=None):\n    # Set seed for reproducibility\n    if random_state:\n        np.random.seed(random_state)\n    \n    # 1. Create a shuffled array of indices\n    shuffled_indices = np.random.permutation(len(X))\n    \n    # 2. Determine the split point\n    test_set_size = int(len(X) * test_size)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    \n    # 3. Split the data using the shuffled indices\n    X_train, X_test = X[train_indices], X[test_indices]\n    y_train, y_test = y[train_indices], y[test_indices]\n    \n    return X_train, X_test, y_train, y_test\n\n# Example usage\nX = np.arange(20).reshape((10, 2))\ny = np.arange(10)\n\nX_train, X_test, y_train, y_test = train_test_split_manual(X, y, random_state=42)\nprint("X_test indices correspond to y_test indices:\\n", X_test, "\\n", y_test)\n---CODE_END---',
            example: '**Code Explanation**:\nThis function avoids a simple sequential split, which can be biased if the data is ordered. \n1. It first creates a randomly shuffled permutation of indices from 0 to N-1, where N is the number of samples. \n2. It then calculates how many indices should be in the test set and splits the shuffled index array. \n3. Finally, it uses these index arrays to select the corresponding rows from the original `X` and `y` data, ensuring that the features and labels remain aligned in both the training and testing sets.'
        },
        {
            id: 'pychal-5',
            question: 'Write a function to compute mean, median, and mode manually.',
            concepts: '**Core Concepts**: Measures of Central Tendency.\n**Explanation**: Mean, median, and mode are the three most common ways to measure the "center" of a dataset.\n- **Mean**: The average value.\n- **Median**: The middle value of a sorted dataset. It is robust to outliers.\n- **Mode**: The most frequently occurring value.',
            answer: '---CODE_START---python\nfrom collections import Counter\n\ndef calculate_stats(data):\n    n = len(data)\n    \n    # 1. Mean\n    mean = sum(data) / n\n    \n    # 2. Median\n    sorted_data = sorted(data)\n    mid_index = n // 2\n    if n % 2 == 0: # Even number of elements\n        median = (sorted_data[mid_index - 1] + sorted_data[mid_index]) / 2\n    else: # Odd number of elements\n        median = sorted_data[mid_index]\n        \n    # 3. Mode\n    counts = Counter(data)\n    max_count = max(counts.values())\n    mode = [key for key, value in counts.items() if value == max_count]\n    \n    return {\'mean\': mean, \'median\': median, \'mode\': mode}\n\ndata = [1, 2, 5, 2, 6, 3, 4, 8, 2, 9]\nstats = calculate_stats(data)\nprint(stats)\n# Output: {\'mean\': 4.2, \'median\': 3.5, \'mode\': [2]}\n---CODE_END---',
            example: '**Code Explanation**:\n- **Mean**: Calculated using the standard formula `sum(data) / len(data)`.\n- **Median**: The code first sorts the data. It then checks if the number of elements is even or odd. If odd, the median is the single middle element. If even, it\'s the average of the two middle elements.\n- **Mode**: `collections.Counter` efficiently creates a frequency map (like a dictionary). The code then finds the highest frequency value and returns all items that have this frequency (as there can be more than one mode).'
        },
        {
            id: 'pychal-6',
            question: 'Write a function to compute a confusion matrix manually.',
            concepts: '**Core Concepts**: Classification Model Evaluation, True/False Positives & Negatives.\n**Explanation**: A confusion matrix is a table that visualizes the performance of a classification model. It shows where the model is getting predictions right and where it\'s getting them wrong (i.e., where it is "confused"). The four quadrants are:\n- **True Positive (TP)**: Correctly predicted positive.\n- **True Negative (TN)**: Correctly predicted negative.\n- **False Positive (FP)**: Incorrectly predicted positive (Type I error).\n- **False Negative (FN)**: Incorrectly predicted negative (Type II error).',
            answer: '---CODE_START---python\nimport numpy as np\n\ndef confusion_matrix_manual(y_true, y_pred):\n    # Assuming binary classification with labels 0 and 1\n    tp, tn, fp, fn = 0, 0, 0, 0\n    \n    for actual, predicted in zip(y_true, y_pred):\n        if actual == 1 and predicted == 1:\n            tp += 1\n        elif actual == 0 and predicted == 0:\n            tn += 1\n        elif actual == 0 and predicted == 1:\n            fp += 1\n        elif actual == 1 and predicted == 0:\n            fn += 1\n            \n    # Matrix format is [[TN, FP], [FN, TP]]\n    return np.array([[tn, fp], [fn, tp]])\n\n# Example\ny_true = [1, 0, 1, 1, 0, 1, 0, 0]\ny_pred = [1, 1, 1, 0, 0, 1, 0, 1]\n\nmatrix = confusion_matrix_manual(y_true, y_pred)\nprint("Confusion Matrix:\\n", matrix)\n# Output:\n# [[TN, FP]\n#  [FN, TP]]\n# [[2, 2]\n#  [1, 3]]\n---CODE_END---',
            example: '**Code Explanation**:\nThe function initializes counters for TP, TN, FP, and FN to zero. It then iterates through the true labels and predicted labels simultaneously using `zip`. Inside the loop, a series of `if/elif` statements compares the `actual` and `predicted` values to determine which of the four outcomes occurred for that pair, and increments the corresponding counter. Finally, it arranges these values into a 2x2 NumPy array, following the standard convention.'
        },
        {
            id: 'pychal-7',
            question: 'Implement linear regression from scratch using only NumPy.',
            concepts: '**Core Concepts**: Machine Learning Fundamentals, Gradient Descent, Cost Function (MSE), Vectorization.\n**Explanation**: This exercise goes to the heart of how many machine learning models "learn". The model makes a prediction, calculates its error (using a cost function like Mean Squared Error), and then uses Gradient Descent to iteratively adjust its internal parameters (weights and bias) in the direction that minimizes this error. Vectorization with NumPy makes these calculations efficient.',
            answer: '---CODE_START---python\nimport numpy as np\n\nclass LinearRegression:\n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        self.lr = learning_rate\n        self.n_iter = n_iterations\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # Gradient Descent\n        for _ in range(self.n_iter):\n            # 1. Make a prediction\n            y_predicted = np.dot(X, self.weights) + self.bias\n            \n            # 2. Compute gradients (derivatives of the cost function)\n            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n            db = (1 / n_samples) * np.sum(y_predicted - y)\n            \n            # 3. Update weights and bias\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db\n\n    def predict(self, X):\n        return np.dot(X, self.weights) + self.bias\n\n# Example\nX = np.array([[1], [2], [3], [4]])\ny = np.array([3, 5, 7, 9]) # y = 2x + 1\nreg = LinearRegression(learning_rate=0.1, n_iterations=1000)\nreg.fit(X, y)\nprint(f"Learned weights: {reg.weights[0]:.2f}, Learned bias: {reg.bias:.2f}")\n# Expected output close to weights=2.0, bias=1.0\n---CODE_END---',
            example: '**Code Explanation**:\nThe `fit` method implements the gradient descent algorithm. In each iteration of the loop:\n1. It calculates the model\'s current predictions (`y_predicted`).\n2. It computes the gradients `dw` (for weights) and `db` (for bias). These gradients represent the direction of steepest ascent of the error function; we want to move in the opposite direction.\n3. It updates the `weights` and `bias` by taking a small step (controlled by `learning_rate`) in the negative direction of the gradient. Over many iterations, this process converges to the optimal parameters that minimize the prediction error.'
        },
        {
            id: 'pychal-13',
            question: 'Convert categorical columns to numeric without libraries.',
            concepts: '**Core Concepts**: Feature Engineering, Manual Encoding.\n**Explanation**: Before the convenience of libraries like scikit-learn, you had to manually implement encoding. This exercise demonstrates the underlying logic: create a mapping from each unique category to an integer, and then apply that mapping to the entire column.',
            answer: '---CODE_START---python\ndef encode_categorical(column_data):\n    # 1. Find unique categories\n    unique_categories = sorted(list(set(column_data)))\n    \n    # 2. Create a mapping from category to integer\n    mapping = {category: i for i, category in enumerate(unique_categories)}\n    \n    # 3. Apply the mapping to the original data\n    encoded_data = [mapping[item] for item in column_data]\n    \n    return encoded_data, mapping\n\ncategories = [\'red\', \'blue\', \'green\', \'red\', \'blue\', \'blue\']\nencoded, a_mapping = encode_categorical(categories)\n\nprint("Original:", categories)\nprint("Encoded:", encoded)\nprint("Mapping:", a_mapping)\n# Output:\n# Original: [\'red\', \'blue\', \'green\', \'red\', \'blue\', \'blue\']\n# Encoded: [2, 0, 1, 2, 0, 0]\n# Mapping: {\'blue\': 0, \'green\': 1, \'red\': 2}\n---CODE_END---',
            example: '**Code Explanation**:\nThe function follows a three-step process:\n1. It finds all unique categories by converting the list to a `set` and back to a list.\n2. It builds a `mapping` dictionary where each unique category is a key and its integer index (from `enumerate`) is the value.\n3. It uses a list comprehension to iterate through the original data and substitute each category string with its corresponding integer from the mapping.'
        },
        {
            id: 'pychal-15',
            question: 'Implement k-means from scratch using NumPy.',
            concepts: '**Core Concepts**: Unsupervised Learning, Clustering, Centroids, Euclidean Distance, Iterative Algorithms.\n**Explanation**: K-Means is a fundamental clustering algorithm. Implementing it from scratch solidifies understanding of its iterative two-step process: (1) Assign each data point to the nearest cluster center (centroid). (2) Update each centroid to be the mean of the points assigned to it. This process repeats until the centroids no longer move.',
            answer: '---CODE_START---python\nimport numpy as np\n\ndef kmeans(X, n_clusters, max_iters=100):\n    # 1. Randomly initialize centroids from the data points\n    centroids = X[np.random.choice(X.shape[0], n_clusters, replace=False)]\n    \n    for _ in range(max_iters):\n        # 2. Assign clusters (Assignment Step)\n        # Calculate distances from each point to each centroid using broadcasting\n        distances = np.sqrt(((X - centroids[:, np.newaxis])**2).sum(axis=2))\n        labels = np.argmin(distances, axis=0)\n        \n        # 3. Update centroids (Update Step)\n        new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(n_clusters)])\n        \n        # 4. Check for convergence\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n            \n    return labels, centroids\n\n# Example\nX = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\nlabels, centroids = kmeans(X, n_clusters=2)\nprint("Cluster Labels:", labels)\nprint("Final Centroids:\\n", centroids)\n---CODE_END---',
            example: '**Code Explanation**:\nThe core logic is in the `for` loop.\n- **Assignment**: The line calculating `distances` is a powerful piece of NumPy broadcasting. It computes the Euclidean distance between every point in `X` and every `centroid` simultaneously, without a Python loop. `np.argmin` then finds the index of the closest centroid for each point.\n- **Update**: A list comprehension calculates the new centroids by filtering `X` for all points belonging to a cluster (`labels == k`) and taking their `mean`.\n- **Convergence**: The loop breaks early if the centroids don\'t change between iterations.'
        },
        {
            id: 'pychal-16',
            question: 'Write a function to normalize/standardize data.',
            concepts: '**Core Concepts**: Feature Scaling, Normalization (Min-Max), Standardization (Z-score).\n**Explanation**: Many ML algorithms perform poorly if input features are on vastly different scales. \n- **Normalization** scales data to a fixed range, usually [0, 1]. It\'s useful when you need bounded values.\n- **Standardization** transforms data to have a mean of 0 and a standard deviation of 1. It\'s less affected by outliers and is the default choice for many algorithms.',
            answer: '---CODE_START---python\nimport numpy as np\n\ndef scale_data(data, method=\'standardize\'):\n    if method == \'normalize\':\n        min_val = np.min(data, axis=0)\n        max_val = np.max(data, axis=0)\n        return (data - min_val) / (max_val - min_val)\n    elif method == \'standardize\':\n        mean_val = np.mean(data, axis=0)\n        std_val = np.std(data, axis=0)\n        return (data - mean_val) / std_val\n    else:\n        raise ValueError("Method must be \'normalize\' or \'standardize\'")\n\ndata = np.array([[10, 100], [20, 200], [30, 300]], dtype=float)\n\nstandardized = scale_data(data, method=\'standardize\')\nnormalized = scale_data(data, method=\'normalize\')\n\nprint("Standardized (Z-score):\\n", standardized)\nprint("\\nNormalized (Min-Max):\\n", normalized)\n---CODE_END---',
            example: '**Code Explanation**:\nThe function takes a NumPy array `data` and a `method` string. The `axis=0` argument in the NumPy functions is crucial; it ensures the calculations (mean, min, max, std) are performed column-wise, so each feature is scaled independently. The code then applies the respective formula for standardization or normalization to the entire array at once using vectorized operations.'
        },
        {
            id: 'pychal-26',
            question: 'Implement the k-nearest neighbors (k-NN) algorithm from scratch in Python.',
            concepts: '**Core Concepts**: Supervised Learning, Classification, Euclidean Distance, Non-parametric model, Lazy Learning.\n**Explanation**: K-Nearest Neighbors (k-NN) is a simple, instance-based learning algorithm. It makes no assumptions about the underlying data distribution (non-parametric). To classify a new, unseen data point, it finds the \'k\' most similar data points (neighbors) in the entire training set and returns the majority class among these neighbors. It\'s called a "lazy" algorithm because it does no training; all the computation happens during prediction.',
            answer: '---CODE_START---python\nimport numpy as np\nfrom collections import Counter\n\ndef euclidean_distance(x1, x2):\n    return np.sqrt(np.sum((x1 - x2)**2))\n\nclass KNN:\n    def __init__(self, k=3):\n        self.k = k\n\n    def fit(self, X_train, y_train):\n        self.X_train = X_train\n        self.y_train = y_train\n\n    def predict(self, X_test):\n        predictions = [self._predict_single(x) for x in X_test]\n        return np.array(predictions)\n\n    def _predict_single(self, x):\n        # 1. Compute distances to all training points\n        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n        \n        # 2. Get the indices of the k-nearest neighbors\n        k_indices = np.argsort(distances)[:self.k]\n        \n        # 3. Get the labels of the k-nearest neighbors\n        k_nearest_labels = [self.y_train[i] for i in k_indices]\n        \n        # 4. Return the most common class label (majority vote)\n        most_common = Counter(k_nearest_labels).most_common(1)\n        return most_common[0][0]\n\n# Example usage\nX_train = np.array([[1, 2], [2, 3], [3, 1], [6, 7], [7, 8], [8, 6]])\ny_train = np.array([0, 0, 0, 1, 1, 1])\nX_test = np.array([[2, 2], [7, 7]])\n\nknn = KNN(k=3)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\nprint(f"Predictions for {X_test}: {predictions}")\n# Output: Predictions for [[2 2] [7 7]]: [0 1]\n---CODE_END---',
            example: '**Code Explanation**:\nThe `KNN` class stores the training data in `fit()`. The main logic is in `_predict_single()`, which is called for each test point. It calculates the Euclidean distance from the test point `x` to every point in `self.X_train`, uses `np.argsort()` to get the indices of the `k` smallest distances, retrieves the labels for those neighbors, and uses `collections.Counter` to find the most frequent label among them.'
        },
        {
            id: 'pychal-27',
            question: 'Implement the Apriori algorithm for association rule mining in Python.',
            concepts: '**Core Concepts**: Unsupervised Learning, Market Basket Analysis, Association Rules, Support, Frequent Itemsets, Candidate Generation.\n**Explanation**: Apriori is a classic algorithm used for mining frequent itemsets and learning association rules. It is based on the "Apriori principle": if an itemset is frequent, then all of its subsets must also be frequent. The algorithm works iteratively: it first finds all frequent itemsets of size 1, then uses them to generate candidate itemsets of size 2, prunes them, and so on, until no more frequent itemsets can be found.',
            answer: '---CODE_START---python\nfrom collections import defaultdict\n\ndef apriori(transactions, min_support):\n    # 1. Get initial frequent itemsets of size 1\n    item_counts = defaultdict(int)\n    for tx in transactions:\n        for item in tx:\n            item_counts[item] += 1\n\n    num_transactions = len(transactions)\n    L1 = {frozenset([item]) for item, count in item_counts.items() \n          if (count / num_transactions) >= min_support}\n\n    L_k_minus_1 = L1\n    frequent_itemsets = list(L1)\n    k = 2\n\n    while L_k_minus_1:\n        # 2. Generate candidates of size k\n        candidates_k = {i.union(j) for i in L_k_minus_1 for j in L_k_minus_1 \n                        if len(i.union(j)) == k}\n\n        # 3. Prune candidates and find frequent itemsets of size k\n        candidate_counts = defaultdict(int)\n        for tx in transactions:\n            for c in candidates_k:\n                if c.issubset(tx):\n                    candidate_counts[c] += 1\n        \n        L_k = {item for item, count in candidate_counts.items() \n               if (count / num_transactions) >= min_support}\n\n        if not L_k:\n            break\n\n        frequent_itemsets.extend(list(L_k))\n        L_k_minus_1 = L_k\n        k += 1\n\n    return frequent_itemsets\n\n# Example\ntransactions = [\n    {\'milk\', \'bread\', \'eggs\'},\n    {\'milk\', \'bread\'},\n    {\'milk\', \'eggs\'},\n    {\'bread\', \'eggs\', \'juice\'}\n]\n\nfrequent = apriori(transactions, min_support=0.5)\nprint("Frequent Itemsets:", frequent)\n---CODE_END---',
            example: '**Code Explanation**:\nThe algorithm starts by calculating the support for individual items to find the first set of frequent itemsets (`L1`). The main `while` loop then iterates to find larger itemsets. In each iteration, it generates `candidates_k` of size `k` by joining frequent itemsets of size `k-1`. It then scans the transaction list again to count the support for these candidates and prunes those below `min_support` to get the next set of frequent itemsets, `L_k`. The process stops when no new frequent itemsets can be found.'
        },
        {
            id: 'pychal-28',
            question: 'Write a Python function to implement logistic regression using gradient descent.',
            concepts: '**Core Concepts**: Supervised Learning, Classification, Sigmoid Function, Log-Loss (Binary Cross-Entropy), Gradient Descent, Vectorization.\n**Explanation**: Logistic regression is a fundamental classification algorithm. It adapts linear regression by squashing the output through a **sigmoid function**, which maps any real value into a range between 0 and 1, representing a probability. The model is trained using an optimization algorithm like **gradient descent** to find the weights that minimize the **Log-Loss (or Binary Cross-Entropy)** cost function, which measures the error between the predicted probabilities and the actual binary labels.',
            answer: '---CODE_START---python\nimport numpy as np\n\nclass LogisticRegression:\n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        self.lr = learning_rate\n        self.n_iter = n_iterations\n        self.weights = None\n        self.bias = None\n\n    def _sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # Gradient Descent\n        for _ in range(self.n_iter):\n            # 1. Calculate linear model and apply sigmoid\n            linear_model = np.dot(X, self.weights) + self.bias\n            y_predicted_proba = self._sigmoid(linear_model)\n            \n            # 2. Compute gradients for log-loss function\n            dw = (1 / n_samples) * np.dot(X.T, (y_predicted_proba - y))\n            db = (1 / n_samples) * np.sum(y_predicted_proba - y)\n            \n            # 3. Update weights and bias\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db\n\n    def predict(self, X):\n        linear_model = np.dot(X, self.weights) + self.bias\n        y_predicted_proba = self._sigmoid(linear_model)\n        # Convert probabilities to class labels (0 or 1)\n        y_predicted_labels = [1 if i > 0.5 else 0 for i in y_predicted_proba]\n        return np.array(y_predicted_labels)\n\n# Example\nX = np.array([[1], [2], [4], [5]])\ny = np.array([0, 0, 1, 1])\nclf = LogisticRegression(learning_rate=0.1, n_iterations=1000)\nclf.fit(X, y)\nprint("Predictions:", clf.predict(X))\n# Expected output close to [0 0 1 1]\n---CODE_END---',
            example: '**Code Explanation**:\nThis class is very similar to the linear regression implementation, with two key differences. First, the `_sigmoid` function is used to transform the linear output into a probability. Second, while the gradient calculation formula looks identical, it is actually the derivative of the log-loss function, not the mean squared error. The `predict` method returns binary class labels by checking if the predicted probability is greater than 0.5.'
        }
    ]
};

export default mlAlgorithmsScratchCategory;
