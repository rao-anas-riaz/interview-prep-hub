import { QuestionCategory } from '../types';

const mlModelingCategory: QuestionCategory = {
    id: 'ml_modeling',
    title: 'Machine Learning Theory',
    icon: 'fa-robot',
    description: 'Core concepts behind how machine learning models work, their strengths, and weaknesses.',
    questions: [
      {
        id: 'foundations-1',
        question: 'What is the difference between AI, ML, Deep Learning, and Data Science?',
        concepts: '**Artificial Intelligence (AI)**: A broad field of computer science concerned with building smart machines capable of performing tasks that typically require human intelligence.\n**Machine Learning (ML)**: A subset of AI that gives computers the ability to learn without being explicitly programmed.\n**Deep Learning (DL)**: A subfield of ML that uses neural networks with many layers (deep neural networks) to learn from large amounts of data. It excels at complex pattern recognition.\n**Data Science (DS)**: An interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.',
        answer: 'Think of them as nested and overlapping concepts:\n- **AI (Artificial Intelligence)**: The broad goal of creating intelligent machines.\n- **ML (Machine Learning)**: A subset of AI that uses data to train algorithms to make predictions.\n- **DL (Deep Learning)**: A subfield of ML that uses complex, multi-layered neural networks. It\'s the engine behind many state-of-the-art AI applications.\n- **DS (Data Science)**: An interdisciplinary field that uses ML (including DL), statistics, and domain knowledge to extract insights from data.',
        example: 'A **self-driving car** is an application of **AI**. The specific system that learns to **identify pedestrians** from camera data is **ML**, and the powerful neural network used for this image recognition is an example of **Deep Learning (DL)**. The entire process of **collecting data, building the model, and analyzing its safety performance** is **Data Science**.',
      },
      {
        id: 'foundations-2',
        question: 'What is the difference between supervised and unsupervised learning?',
        concepts: '**Supervised Learning**: Machine learning tasks that learn a function that maps an input to an output based on example input-output pairs. It uses **labeled data**.\n**Unsupervised Learning**: Machine learning tasks that infer patterns from a dataset without reference to known, or labeled, outcomes.',
        answer: 'The difference is the data you use.\n- **Supervised Learning**: Uses **labeled data** (data with the "right answer"). Its goal is to **predict an outcome**.\n- **Unsupervised Learning**: Uses **unlabeled data** (no "right answer" provided). Its goal is to **discover hidden patterns**.',
        example: '- **Supervised**: Predicting a house price (a specific number) based on **labeled** historical sales data.\n- **Unsupervised**: Grouping customers into different segments based on their purchase history, **without any predefined labels** for those groups.',
      },
      {
        id: 'foundations-8',
        question: 'Explain the Bias-Variance Tradeoff as if you\'re teaching a new analyst.',
        concepts: '**Bias**: The error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n**Variance**: The error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).',
        answer: 'It\'s a balancing act to prevent a model from being too simple or too complex.\n- **High Bias (Underfitting)**: The model is **too simple**. It misses the real patterns and is inaccurate on both training and test data.\n- **High Variance (Overfitting)**: The model is **too complex**. It learns the noise, not the pattern. It\'s great on training data but fails on new, unseen data.\n\nThe goal is to find the sweet spot: a model with **low bias and low variance** that generalizes well.',
        example: '**When predicting house prices**:\n- A model using only "**number of rooms**" is too simple (**high bias**).\n- A model using 500 features including "**front door color**" is too complex (**high variance**).\n- A model using key features like "**square footage**," "**location**," and "**age**" is the right balance.',
      },
      {
        id: 'ml-1',
        question: 'What is the difference between Linear Regression and Logistic Regression?',
        concepts: '**Linear Regression**: A model for predicting a **continuous** target variable.\n**Logistic Regression**: A model for predicting a **categorical** (binary) target variable.',
        answer: 'The key difference is the **type of output** they predict.\n- **Linear Regression** predicts a **continuous value**. Its output can be any number (e.g., price, temperature).\n- **Logistic Regression** predicts a **probability** that an input belongs to a certain class. Its output is always between 0 and 1, which is then mapped to a class (e.g., Yes/No, True/False).',
        example: 'Use **Linear Regression** to predict a house\'s **price in dollars**. Use **Logistic Regression** to predict **whether or not a customer will churn** (a yes/no outcome).',
      },
      {
        id: 'ml-2',
        question: 'What happens if you apply a linear regression model to a non-linear dataset?',
        concepts: '**Linear Regression**: A statistical model that assumes a linear relationship between the independent and dependent variables.\n**Non-linear Data**: Data where the relationship between variables cannot be represented by a straight line.',
        answer: 'Applying a linear regression model to inherently non-linear data will result in a model with **high bias (underfitting)**. The model will fail to capture the underlying trend, leading to **poor predictive performance** on both the training and test sets. The straight line of the model will be a poor approximation of the curved pattern in the data.',
        example: 'Imagine trying to predict a company\'s user growth, which follows an **exponential curve**. A linear regression model would just draw a **straight line** through the curve, severely underestimating future growth and overestimating past growth. A more appropriate model would be a polynomial regression or another non-linear model that can capture the curve.',
      },
      {
        id: 'ml-3',
        question: 'How does a Decision Tree work?',
        concepts: '**Decision Tree**: A non-parametric supervised learning method used for classification and regression. It is a tree-like model of decisions where each internal node represents a "test" on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label or continuous value.',
        answer: 'A decision tree makes predictions by learning a series of **if/else questions** about the features. It recursively splits the data on the feature that best separates the target variable, aiming to create "pure" leaf nodes (nodes containing mostly one class).\n\nFor example, to predict if someone will play tennis, it might ask:\n1. Is the **outlook sunny**? (Yes/No)\n2. Is the **humidity high**? (Yes/No)\n3. Is the **wind strong**? (Yes/No)\nBy following the path down the tree, you arrive at a final prediction.',
        example: 'A bank uses a decision tree to classify loan applications. The first split might be on **`credit_score > 700`**. The next split under the "Yes" branch might be on **`income > $50,000`**. Each leaf node would be a final decision: "Approve" or "Deny".',
      },
      {
        id: 'ml-4',
        question: 'What are the main differences between Bagging and Boosting?',
        concepts: '**Bagging (Bootstrap Aggregating)**: An ensemble technique where multiple independent models are trained **in parallel** on different random subsets of the training data. Their predictions are then averaged to produce a final result. The goal is to **reduce variance**.\n**Boosting**: An ensemble technique where models are trained **sequentially**. Each new model focuses on correcting the errors made by the previous ones. The goal is to **reduce bias**.',
        answer: 'Both combine many "weak" decision trees into one "strong" model, but they do it differently.\n- **Bagging** (e.g., Random Forest): Trains trees **in parallel** on random data samples. It averages their predictions to **reduce variance**.\n- **Boosting** (e.g., XGBoost): Trains trees **sequentially**. Each new tree focuses on fixing the mistakes of the previous one to **reduce bias**.',
        example: '**Bagging** is like getting a second opinion from many independent doctors. **Boosting** is like seeing a series of specialists, where each one refines the diagnosis of the last.',
      },
      {
        id: 'ml-5',
        question: 'Explain Random Forest. Why is it said to reduce variance?',
        concepts: '**Random Forest**: An ensemble learning method that operates by constructing a multitude of decision trees at training time. It uses bagging and feature randomness.\n**Variance**: The error from sensitivity to small fluctuations in the training set.',
        answer: 'This is a common misconception. The primary strength of Random Forest is that it significantly **reduces *variance*, not bias**. Individual decision trees in the ensemble are typically deep, meaning they have **low bias but very high variance** (they overfit to their specific subset of data). By averaging the predictions of many different, uncorrelated trees (thanks to bagging and feature randomness), Random Forest cancels out the noise and **reduces this high variance**, resulting in a robust model that generalizes well.',
        example: 'Think of one deep decision tree as a single, **overconfident expert** who has studied a small part of a problem. They might have a low-bias view but are highly variable. A Random Forest is like a **committee of these experts**. Each has their own high-variance opinion, but by averaging their votes, the committee\'s final decision is much more stable, reliable, and has **lower overall variance** than any single expert.',
      },
       {
        id: 'ml-6',
        question: 'What is Gradient Boosting and what makes XGBoost so powerful?',
        concepts: '**Gradient Boosting**: A machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.\n**XGBoost**: An optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.',
        answer: 'Gradient Boosting is a powerful boosting algorithm where new trees are trained to correct the **residuals (errors)** of the previous trees. It uses gradient descent to minimize the loss when adding new models.\n\n**XGBoost (Extreme Gradient Boosting)** is a popular implementation that is so effective because of:\n- **Regularization**: It has built-in L1 and L2 regularization, which helps prevent overfitting.\n- **Parallel Processing**: It can train on multiple CPU cores, making it much faster.\n- **Handling Missing Values**: It has a smart, built-in way to handle missing data.',
        example: 'XGBoost is often the winning algorithm in Kaggle competitions. For a problem like predicting customer churn, XGBoost can effectively model complex interactions between features like `monthly_charges`, `tenure`, and `contract_type` to achieve state-of-the-art accuracy.',
      },
      {
        id: 'ml-7',
        question: 'Logistic Regression vs. Random Forest â€“ how do you pick one for a real business problem?',
        concepts: '**Logistic Regression**: A linear model for binary classification that is highly interpretable.\n**Random Forest**: An ensemble of decision trees that is non-linear, generally more powerful, but less interpretable.',
        answer: 'The choice is a tradeoff between **interpretability** and **predictive power**.\n- Use **Logistic Regression** when you need to **explain** the results. Its coefficients tell you the "why" behind a prediction.\n- Use **Random Forest** when you need the **best possible accuracy**. It automatically handles complex, non-linear patterns but is harder to interpret.',
        example: '**A bank uses Logistic Regression** for loan decisions because they must legally explain why someone was denied. **An e-commerce site uses Random Forest** for product recommendations because showing the most relevant item is more important than explaining the choice.',
      },
      {
        id: 'ml-8',
        question: 'How do you know your model is overfitting without looking at code?',
        concepts: '**Overfitting**: A modeling error that occurs when a function is too closely fit to a limited set of data points. An overfit model has high variance and low bias.\n**Generalization**: The ability of a model to adapt properly to new, previously unseen data, drawn from the same distribution as the one used to create the model.',
        answer: 'The classic sign is a **large performance gap between your training and test data**.\n- If your model has **99% accuracy on the training set** (data it has seen before)... \n- ...but **only 75% accuracy on the test set** (new data)... \n...it is **overfitting**. It has "memorized" the training data instead of learning a general rule.',
        example: 'You build a model to predict house prices. On your training data, its error is only **$1,000**. On new houses in the test set, its error jumps to **$50,000**. This massive drop in performance indicates overfitting.',
      },
       {
        id: 'ml-9',
        question: 'What is regularization and why is it useful?',
        concepts: '**Regularization**: A technique used to prevent overfitting by adding a penalty term to the model\'s loss function. L1 (Lasso) and L2 (Ridge) are the most common types.',
        answer: '**Regularization** is a technique to **prevent overfitting** by penalizing model complexity. It keeps the model from learning noise.\n- **L1 Regularization (Lasso)**: Can shrink feature coefficients to **exactly zero**, effectively acting as a form of **automatic feature selection**.\n- **L2 Regularization (Ridge)**: Shrinks coefficients **towards zero** but not completely. It is useful for handling correlated features.',
        example: 'In a model with 100 features, **L1 regularization** might find that only 20 are truly predictive, setting the coefficients for the other 80 to zero. This simplifies the model and makes it more robust.',
      },
      {
        id: 'ml-10',
        question: 'What is dimensionality reduction and why is it useful?',
        concepts: '**Dimensionality Reduction**: The process of reducing the number of random variables or features under consideration, via obtaining a set of principal variables. The most common technique is PCA.',
        answer: 'It\'s the process of **reducing the number of features** in a dataset while retaining as much important information as possible.\n\nWhy it\'s useful:\n- **Prevents the "curse of dimensionality"** and reduces overfitting.\n- **Speeds up model training**.\n- **Allows for visualization** of high-dimensional data.',
        example: 'A dataset of customer behavior might have 500 features. Using **Principal Component Analysis (PCA)**, you could reduce this to 20 "principal components" that capture the most important patterns, making it much easier and faster to train a churn prediction model.',
      },
      {
        id: 'ml-11',
        question: 'Explain Principal Component Analysis (PCA) and its main steps.',
        concepts: '**Principal Components**: New, uncorrelated variables that are linear combinations of the original variables.\n**Variance**: PCA aims to maximize the variance captured by each principal component.\n**Eigenvectors/Eigenvalues**: The mathematical basis for PCA. Eigenvectors of the covariance matrix are the principal components.',
        answer: 'PCA is a **dimensionality reduction** technique used to transform a large set of correlated variables into a smaller set of uncorrelated variables called **principal components**, while retaining most of the information (variance) from the original dataset.\n\nThe main steps are:\n1.  **Standardize the Data**: Scale the features to have a mean of 0 and a standard deviation of 1. This is crucial because PCA is sensitive to the variance of the initial variables.\n2.  **Compute the Covariance Matrix**: This matrix shows the correlations between all pairs of variables.\n3.  **Calculate Eigenvectors and Eigenvalues**: The eigenvectors of the covariance matrix are the directions of the new feature space (the principal components), and the eigenvalues are their magnitudes. The first principal component is the eigenvector with the highest eigenvalue and captures the most variance.\n4.  **Select Principal Components**: Sort the eigenvalues in descending order and choose the top `k` eigenvectors to form a new, smaller feature matrix.',
        example: '**Image Compression**: A high-resolution image has many correlated pixels. PCA can reduce the number of pixels (dimensions) needed to represent the image while preserving its essential visual information. Another example is reducing 10 highly correlated economic indicators into 2 or 3 "economic health" principal components for a simpler model.',
      },
      {
        id: 'ml-12',
        question: 'What is a Support Vector Machine (SVM) and how does the kernel trick work?',
        concepts: '**Hyperplane**: The decision boundary used to separate classes.\n**Margin**: The distance between the hyperplane and the nearest data points from either class.\n**Support Vectors**: The data points that lie closest to the decision surface; they are the most critical points in defining the hyperplane.\n**Kernel Trick**: A method to handle non-linearly separable data by mapping it to a higher-dimensional space where it becomes linearly separable.',
        answer: 'A Support Vector Machine (SVM) is a powerful supervised learning algorithm used for classification. Its goal is to find the optimal **hyperplane** that best separates the data into classes. The "optimal" hyperplane is the one that has the **maximum margin** between the classes.\n\nThe data points that are closest to this hyperplane are called **support vectors** because they are the critical elements that "support" or define the position of the hyperplane.\n\n**The Kernel Trick**:\nFor data that is not linearly separable, SVMs can use the kernel trick. Instead of explicitly transforming the data into a very high-dimensional space (which would be computationally expensive), a kernel function computes the dot product of the data points in that higher-dimensional space without ever creating the new features. This allows SVMs to learn complex, **non-linear decision boundaries** efficiently.',
        example: 'Imagine you have data points in a circular pattern (e.g., one class in the center, another surrounding it). A straight line cannot separate them. A common kernel (the Radial Basis Function or RBF kernel) can map this data to a higher dimension where a simple plane can easily separate the two classes, creating a circular decision boundary back in the original 2D space.',
      },
      {
        id: 'ml-13',
        question: 'What is a Convolutional Neural Network (CNN) and what are its key layers?',
        concepts: '**Convolutional Layer**: Uses filters (kernels) to slide across the input image and detect features like edges, corners, and textures.\n**Pooling Layer**: Downsamples the feature maps, reducing their dimensionality and making the model more robust to variations in the position of features.\n**Fully Connected Layer**: A traditional neural network layer, typically at the end of a CNN, that performs the final classification based on the high-level features detected by earlier layers.',
        answer: 'A Convolutional Neural Network (CNN or ConvNet) is a type of deep learning model that is specifically designed for processing grid-like data, such as images. CNNs are highly effective because they automatically and adaptively learn a hierarchy of spatial features.\n\nThe key layers are:\n1.  **Convolutional Layer**: The core building block. It applies a set of learnable filters to the input image. Each filter is a small matrix that detects a specific feature (e.g., a vertical edge). The layer slides this filter over the entire image to produce a "feature map" that highlights where that feature appears.\n2.  **Pooling (or Subsampling) Layer**: This layer reduces the spatial size of the feature maps, which reduces the number of parameters and computation in the network. The most common type is Max Pooling, which takes the maximum value from a small window of the feature map.\n3.  **Fully Connected Layer**: After several convolutional and pooling layers have extracted high-level features, these features are flattened into a 1D vector and fed into one or more fully connected layers, which perform the final classification task (e.g., deciding if the image contains a "cat" or a "dog").',
        example: 'In an image of a car, the first convolutional layers might learn to detect simple edges. The next layers would combine these edges to detect corners and shapes. Deeper layers might combine those to detect wheels and windows, and the final fully connected layer would use the presence of these complex features to classify the image as a "car".',
      },
      {
        id: 'ml-14',
        question: 'What are Recurrent Neural Networks (RNNs) and what problem do they solve? Mention common variants like LSTM and GRU.',
        concepts: '**Sequential Data**: Data where the order of elements is important, such as time series or text.\n**Hidden State**: The "memory" of an RNN that carries information from previous time steps to the current one.\n**Vanishing Gradient Problem**: A difficulty in training deep or recurrent networks where the gradients used to update the network become extremely small, effectively stopping the learning process for earlier layers/steps.\n**LSTM (Long Short-Term Memory) & GRU (Gated Recurrent Unit)**: Advanced RNN architectures that use "gates" to control the flow of information, overcoming the vanishing gradient problem.',
        answer: 'Recurrent Neural Networks (RNNs) are a class of neural networks designed to work with **sequential data**. Unlike standard feedforward networks, RNNs have loops, allowing them to persist information. This "memory" is stored in a **hidden state** that is passed from one step in the sequence to the next.\n\nHowever, basic RNNs suffer from the **vanishing gradient problem**, which makes it very difficult for them to learn long-range dependencies in a sequence.\n\nTo solve this, advanced variants were developed:\n- **LSTM (Long Short-Term Memory)**: A type of RNN that uses a more complex cell structure with three "gates" (input, forget, and output). These gates regulate what information is stored, updated, and read from the cell\'s long-term memory, allowing it to remember important information over long sequences.\n- **GRU (Gated Recurrent Unit)**: A simpler variant of LSTM with only two gates (update and reset). It is often computationally more efficient than LSTM and can perform just as well on many tasks.',
        example: 'When translating the sentence "The cats, which were hungry, sat on the mat," an RNN needs to remember that "cats" was plural to correctly conjugate the verb "were" later in the sentence. An LSTM or GRU is particularly good at handling this kind of long-distance dependency.',
      },
      {
        id: 'ml-15',
        question: 'What is the Expectation-Maximization (EM) algorithm and where is it used?',
        concepts: '**Unsupervised Learning**: Learning from data without labeled outcomes.\n**Latent Variables**: Hidden variables that are not directly observed but are inferred from other variables that are observed.\n**Maximum Likelihood Estimation**: A method of estimating the parameters of a statistical model by finding the parameter values that maximize the likelihood of making the observations given the parameters.',
        answer: 'The Expectation-Maximization (EM) algorithm is an iterative method for finding maximum likelihood estimates of parameters in a statistical model, particularly when the model depends on unobserved **latent variables**.\n\nIt works by alternating between two steps:\n1.  **E-step (Expectation)**: Given the current model parameters, it calculates the "best guess" or expectation for the latent variables. This step essentially fills in the missing data.\n2.  **M-step (Maximization)**: Using the "completed" data from the E-step, it re-estimates the model parameters to maximize the likelihood of the data.\n\nThese two steps are repeated until the parameters converge.',
        example: 'The most common application of EM is in **Gaussian Mixture Models (GMMs)** for clustering. The latent variable is the cluster assignment for each data point. The E-step calculates the probability of each point belonging to each cluster. The M-step then uses these probabilities to update the mean, variance, and weight of each cluster (Gaussian distribution). This process refines the clusters with each iteration.',
      },
    ],
};

export default mlModelingCategory;
