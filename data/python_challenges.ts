import { QuestionCategory } from '../types';

const pythonChallengesCategory: QuestionCategory = {
    id: 'python_challenges',
    title: 'Coding Challenges',
    icon: 'fa-code',
    description: 'Practical, hands-on coding exercises to implement common data science tasks and algorithms from scratch.',
    questions: [
        {
            id: 'pychal-1',
            question: 'Read a CSV, clean missing values, and output summary stats.',
            concepts: '**Core Concepts**: Data Ingestion, Data Cleaning (Imputation), and Descriptive Statistics.\n**Explanation**: This is the foundational workflow for nearly any data analysis task. You must first load the data, then handle imperfections like missing values to ensure your analysis is accurate, and finally, generate summary statistics to get a high-level understanding of the dataset\'s properties.',
            answer: '---CODE_START---python\nimport pandas as pd\nimport numpy as np\n\n# Create a dummy CSV for the example\ndata = {\'A\': [1, 2, np.nan, 4], \'B\': [5, np.nan, 7, 8], \'C\': [\'x\', \'y\', \'y\', \'z\']}\ndf = pd.DataFrame(data)\ndf.to_csv(\'sample.csv\', index=False)\n\n# --- Solution ---\ndef process_csv(filepath):\n    # 1. Read the CSV into a pandas DataFrame\n    df = pd.read_csv(filepath)\n    \n    # 2. Clean missing values\n    # For numeric columns, fill with the median (robust to outliers)\n    for col in df.select_dtypes(include=np.number).columns:\n        df[col].fillna(df[col].median(), inplace=True)\n    \n    # For categorical/object columns, fill with the mode (most frequent value)\n    for col in df.select_dtypes(include=\'object\').columns:\n        df[col].fillna(df[col].mode()[0], inplace=True)\n        \n    # 3. Output summary statistics\n    # describe() provides stats for numeric columns by default\n    # include=\'all\' shows stats for both numeric and categorical columns\n    summary = df.describe(include=\'all\')\n    return df, summary\n\ncleaned_df, summary_stats = process_csv(\'sample.csv\')\nprint("Cleaned DataFrame:\\n", cleaned_df)\nprint("\\nSummary Statistics:\\n", summary_stats)\n---CODE_END---',
            example: '**Code Explanation**:\nThe `process_csv` function automates a standard cleaning pipeline. It first identifies numeric columns (`select_dtypes(include=np.number)`) and fills any `NaN` values with the column\'s median. It then does the same for non-numeric columns (`object`), but uses the mode. This separation is crucial because you can\'t calculate the median of a string. Finally, `df.describe(include=\'all\')` generates key statistics like mean, count, standard deviation for numeric columns, and count, unique values, and frequency for categorical columns.'
        },
        {
            id: 'pychal-2',
            question: 'Merge two datasets on multiple keys.',
            concepts: '**Core Concepts**: Relational Algebra, Database-style Joins.\n**Explanation**: In real-world data, a single column is often not enough to uniquely link two tables. For instance, you might need to join data based on both a `user_id` and a `transaction_date`. Merging on multiple keys allows you to perform these more complex, precise joins.',
            answer: '---CODE_START---python\nimport pandas as pd\n\ndf_left = pd.DataFrame({\n    \'key1\': [\'K0\', \'K0\', \'K1\', \'K2\'],\n    \'key2\': [\'K0\', \'K1\', \'K0\', \'K1\'],\n    \'A\': [\'A0\', \'A1\', \'A2\', \'A3\']\n})\n\ndf_right = pd.DataFrame({\n    \'key1\': [\'K0\', \'K1\', \'K1\', \'K2\'],\n    \'key2\': [\'K0\', \'K0\', \'K0\', \'K0\'],\n    \'B\': [\'B0\', \'B1\', \'B2\', \'B3\']\n})\n\n# Perform an inner merge on the combination of \'key1\' and \'key2\'\nmerged_df = pd.merge(df_left, df_right, on=[\'key1\', \'key2\'], how=\'inner\')\n\nprint("Merged DataFrame (on [key1, key2]):\\n", merged_df)\n---CODE_END---',
            example: '**Code Explanation**:\nThe `pd.merge()` function is pandas\' primary tool for joining DataFrames. By passing a list of column names to the `on` parameter (e.g., `on=[\'key1\', \'key2\']`), you instruct pandas to find rows where the values in **both** specified key columns match in `df_left` and `df_right`. The `how=\'inner\'` argument ensures that only rows with matching key combinations in both DataFrames are included in the final result.'
        },
        {
            id: 'pychal-3',
            question: 'Create pivot tables and aggregated summaries.',
            concepts: '**Core Concepts**: Data Reshaping, Aggregation, Cross-Tabulation.\n**Explanation**: Datasets are often in a "long" format, where each row is a single observation. A pivot table reshapes or transforms this data into a "wide" format, which is often more human-readable and useful for analysis. It summarizes data by grouping categories along rows and columns and applying an aggregation function (like mean, sum, or count) to the values.',
            answer: '---CODE_START---python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    "Region": ["North", "North", "South", "South", "North", "South"],\n    "Product": ["A", "B", "A", "B", "A", "B"],\n    "Sales": [100, 150, 200, 50, 80, 120],\n    "Quantity": [10, 12, 15, 8, 9, 11]\n})\n\n# Create a pivot table\n# Index: Rows of the pivot table\n# Columns: Columns of the pivot table\n# Values: The data to aggregate\n# Aggfunc: The aggregation function to apply\npivot = pd.pivot_table(df, \n                       values=["Sales", "Quantity"], \n                       index=["Region"], \n                       columns=["Product"], \n                       aggfunc={\'Sales\': np.sum, \'Quantity\': np.mean})\n\nprint("Pivot Table:\\n", pivot)\n---CODE_END---',
            example: '**Code Explanation**:\n`pd.pivot_table` is a powerful function for summarization. In this example:\n- `index=["Region"]` sets "North" and "South" as the rows.\n- `columns=["Product"]` sets "A" and "B" as the main columns.\n- `values=["Sales", "Quantity"]` specifies the numerical data we want to analyze.\n- `aggfunc` applies different functions to different value columns: it calculates the `sum` of `Sales` and the `mean` of `Quantity` for each Region/Product combination.'
        },
        {
            id: 'pychal-4',
            question: 'Implement your own train-test split without sklearn.',
            concepts: '**Core Concepts**: Model Validation, Overfitting, Shuffling, Reproducibility.\n**Explanation**: Splitting your data into a training set (for the model to learn from) and a test set (for evaluation) is the most fundamental step in model validation. It allows you to assess how well your model generalizes to new, unseen data. Without it, you risk overfitting, where the model simply memorizes the training data and performs poorly in the real world.',
            answer: '---CODE_START---python\nimport numpy as np\n\ndef train_test_split_manual(X, y, test_size=0.2, random_state=None):\n    # Set seed for reproducibility\n    if random_state:\n        np.random.seed(random_state)\n    \n    # 1. Create a shuffled array of indices\n    shuffled_indices = np.random.permutation(len(X))\n    \n    # 2. Determine the split point\n    test_set_size = int(len(X) * test_size)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    \n    # 3. Split the data using the shuffled indices\n    X_train, X_test = X[train_indices], X[test_indices]\n    y_train, y_test = y[train_indices], y[test_indices]\n    \n    return X_train, X_test, y_train, y_test\n\n# Example usage\nX = np.arange(20).reshape((10, 2))\ny = np.arange(10)\n\nX_train, X_test, y_train, y_test = train_test_split_manual(X, y, random_state=42)\nprint("X_test indices correspond to y_test indices:\\n", X_test, "\\n", y_test)\n---CODE_END---',
            example: '**Code Explanation**:\nThis function avoids a simple sequential split, which can be biased if the data is ordered. \n1. It first creates a randomly shuffled permutation of indices from 0 to N-1, where N is the number of samples. \n2. It then calculates how many indices should be in the test set and splits the shuffled index array. \n3. Finally, it uses these index arrays to select the corresponding rows from the original `X` and `y` data, ensuring that the features and labels remain aligned in both the training and testing sets.'
        },
        {
            id: 'pychal-5',
            question: 'Write a function to compute mean, median, and mode manually.',
            concepts: '**Core Concepts**: Measures of Central Tendency.\n**Explanation**: Mean, median, and mode are the three most common ways to measure the "center" of a dataset.\n- **Mean**: The average value.\n- **Median**: The middle value of a sorted dataset. It is robust to outliers.\n- **Mode**: The most frequently occurring value.',
            answer: '---CODE_START---python\nfrom collections import Counter\n\ndef calculate_stats(data):\n    n = len(data)\n    \n    # 1. Mean\n    mean = sum(data) / n\n    \n    # 2. Median\n    sorted_data = sorted(data)\n    mid_index = n // 2\n    if n % 2 == 0: # Even number of elements\n        median = (sorted_data[mid_index - 1] + sorted_data[mid_index]) / 2\n    else: # Odd number of elements\n        median = sorted_data[mid_index]\n        \n    # 3. Mode\n    counts = Counter(data)\n    max_count = max(counts.values())\n    mode = [key for key, value in counts.items() if value == max_count]\n    \n    return {\'mean\': mean, \'median\': median, \'mode\': mode}\n\ndata = [1, 2, 5, 2, 6, 3, 4, 8, 2, 9]\nstats = calculate_stats(data)\nprint(stats)\n# Output: {\'mean\': 4.2, \'median\': 3.5, \'mode\': [2]}\n---CODE_END---',
            example: '**Code Explanation**:\n- **Mean**: Calculated using the standard formula `sum(data) / len(data)`.\n- **Median**: The code first sorts the data. It then checks if the number of elements is even or odd. If odd, the median is the single middle element. If even, it\'s the average of the two middle elements.\n- **Mode**: `collections.Counter` efficiently creates a frequency map (like a dictionary). The code then finds the highest frequency value and returns all items that have this frequency (as there can be more than one mode).'
        },
        {
            id: 'pychal-6',
            question: 'Write a function to compute a confusion matrix manually.',
            concepts: '**Core Concepts**: Classification Model Evaluation, True/False Positives & Negatives.\n**Explanation**: A confusion matrix is a table that visualizes the performance of a classification model. It shows where the model is getting predictions right and where it\'s getting them wrong (i.e., where it is "confused"). The four quadrants are:\n- **True Positive (TP)**: Correctly predicted positive.\n- **True Negative (TN)**: Correctly predicted negative.\n- **False Positive (FP)**: Incorrectly predicted positive (Type I error).\n- **False Negative (FN)**: Incorrectly predicted negative (Type II error).',
            answer: '---CODE_START---python\nimport numpy as np\n\ndef confusion_matrix_manual(y_true, y_pred):\n    # Assuming binary classification with labels 0 and 1\n    tp, tn, fp, fn = 0, 0, 0, 0\n    \n    for actual, predicted in zip(y_true, y_pred):\n        if actual == 1 and predicted == 1:\n            tp += 1\n        elif actual == 0 and predicted == 0:\n            tn += 1\n        elif actual == 0 and predicted == 1:\n            fp += 1\n        elif actual == 1 and predicted == 0:\n            fn += 1\n            \n    # Matrix format is [[TN, FP], [FN, TP]]\n    return np.array([[tn, fp], [fn, tp]])\n\n# Example\ny_true = [1, 0, 1, 1, 0, 1, 0, 0]\ny_pred = [1, 1, 1, 0, 0, 1, 0, 1]\n\nmatrix = confusion_matrix_manual(y_true, y_pred)\nprint("Confusion Matrix:\\n", matrix)\n# Output:\n# [[TN, FP]\n#  [FN, TP]]\n# [[2, 2]\n#  [1, 3]]\n---CODE_END---',
            example: '**Code Explanation**:\nThe function initializes counters for TP, TN, FP, and FN to zero. It then iterates through the true labels and predicted labels simultaneously using `zip`. Inside the loop, a series of `if/elif` statements compares the `actual` and `predicted` values to determine which of the four outcomes occurred for that pair, and increments the corresponding counter. Finally, it arranges these values into a 2x2 NumPy array, following the standard convention.'
        },
        {
            id: 'pychal-7',
            question: 'Implement linear regression from scratch using only NumPy.',
            concepts: '**Core Concepts**: Machine Learning Fundamentals, Gradient Descent, Cost Function (MSE), Vectorization.\n**Explanation**: This exercise goes to the heart of how many machine learning models "learn". The model makes a prediction, calculates its error (using a cost function like Mean Squared Error), and then uses Gradient Descent to iteratively adjust its internal parameters (weights and bias) in the direction that minimizes this error. Vectorization with NumPy makes these calculations efficient.',
            answer: '---CODE_START---python\nimport numpy as np\n\nclass LinearRegression:\n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        self.lr = learning_rate\n        self.n_iter = n_iterations\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # Gradient Descent\n        for _ in range(self.n_iter):\n            # 1. Make a prediction\n            y_predicted = np.dot(X, self.weights) + self.bias\n            \n            # 2. Compute gradients (derivatives of the cost function)\n            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n            db = (1 / n_samples) * np.sum(y_predicted - y)\n            \n            # 3. Update weights and bias\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db\n\n    def predict(self, X):\n        return np.dot(X, self.weights) + self.bias\n\n# Example\nX = np.array([[1], [2], [3], [4]])\ny = np.array([3, 5, 7, 9]) # y = 2x + 1\nreg = LinearRegression(learning_rate=0.1, n_iterations=1000)\nreg.fit(X, y)\nprint(f"Learned weights: {reg.weights[0]:.2f}, Learned bias: {reg.bias:.2f}")\n# Expected output close to weights=2.0, bias=1.0\n---CODE_END---',
            example: '**Code Explanation**:\nThe `fit` method implements the gradient descent algorithm. In each iteration of the loop:\n1. It calculates the model\'s current predictions (`y_predicted`).\n2. It computes the gradients `dw` (for weights) and `db` (for bias). These gradients represent the direction of steepest ascent of the error function; we want to move in the opposite direction.\n3. It updates the `weights` and `bias` by taking a small step (controlled by `learning_rate`) in the negative direction of the gradient. Over many iterations, this process converges to the optimal parameters that minimize the prediction error.'
        },
        {
            id: 'pychal-8',
            question: 'Write code to detect outliers using z-score or IQR.',
            concepts: '**Core Concepts**: Outlier Detection, Statistical Rules.\n**Explanation**: Outliers can distort statistical analyses and model training. \n- **IQR Method**: A non-parametric method that defines outliers as points that fall a certain distance (typically 1.5 times the Interquartile Range) below the first quartile or above the third quartile.\n- **Z-score Method**: A parametric method that assumes a normal distribution. It measures how many standard deviations a data point is from the mean. A common threshold for outliers is a Z-score greater than 3 or less than -3.',
            answer: '---CODE_START---python\nimport numpy as np\n\ndef detect_outliers(data, method=\'iqr\', threshold=3.0):\n    if method == \'iqr\':\n        Q1 = np.percentile(data, 25)\n        Q3 = np.percentile(data, 75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        outliers = (data < lower_bound) | (data > upper_bound)\n    elif method == \'zscore\':\n        mean = np.mean(data)\n        std = np.std(data)\n        z_scores = (data - mean) / std\n        outliers = np.abs(z_scores) > threshold\n    else:\n        raise ValueError("Method not supported. Use \'iqr\' or \'zscore\'.")\n    \n    return data[outliers]\n\ndata = np.array([1, 2, 2, 3, 3, 4, 4, 5, 5, 15, -10])\nprint("Outliers (IQR):", detect_outliers(data, method=\'iqr\'))\nprint("Outliers (Z-score):", detect_outliers(data, method=\'zscore\'))\n---CODE_END---',
            example: '**Code Explanation**:\nThe function uses NumPy for efficient calculations. \n- For the **\'iqr\' method**, it calculates the 25th and 75th percentiles to find the IQR, defines the upper and lower bounds based on the 1.5*IQR rule, and uses boolean indexing to return the values outside these bounds. \n- For the **\'zscore\' method**, it calculates the mean and standard deviation, computes the Z-score for every point, and returns the points whose absolute Z-score exceeds the threshold.'
        },
        {
            id: 'pychal-9',
            question: 'Write a custom groupby aggregation (e.g., weighted average).',
            concepts: '**Core Concepts**: Custom Aggregations, Split-Apply-Combine.\n**Explanation**: While pandas `groupby` has built-in aggregations like `mean()` and `sum()`, you often need to perform a custom calculation, like a weighted average. The `.apply()` method is perfect for this, as it passes each group as a DataFrame to a custom function you define.',
            answer: '---CODE_START---python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    \'Category\': [\'A\', \'A\', \'B\', \'B\'],\n    \'Value\': [10, 20, 30, 40],\n    \'Weight\': [0.8, 0.2, 0.5, 0.5]\n})\n\n# Define a function to calculate the weighted average\ndef weighted_average(group):\n    return np.average(group[\'Value\'], weights=group[\'Weight\'])\n\n# Group by category and apply the custom function\nresult = df.groupby(\'Category\').apply(weighted_average)\n\nprint("Original DataFrame:\\n", df)\nprint("\\nWeighted Average by Category:\\n", result)\n---CODE_END---',
            example: '**Code Explanation**:\nFirst, we define a function `weighted_average` that takes a DataFrame `group` as input. Inside, we use `np.average`, which can compute a weighted average directly. Then, `df.groupby(\'Category\').apply(weighted_average)` performs the split-apply-combine logic: it splits `df` into groups for categories \'A\' and \'B\', passes each group\'s sub-DataFrame to our `weighted_average` function, and combines the single-value results into a new pandas Series.'
        },
        {
            id: 'pychal-10',
            question: 'Flatten a nested list in Python.',
            concepts: '**Core Concepts**: Recursion, Generators, Iterators.\n**Explanation**: Flattening is the process of converting a list of lists (or a list with multiple levels of nesting) into a single, one-dimensional list. A recursive generator is a highly efficient and elegant way to solve this for lists with unknown or varying depths of nesting. It processes items one by one without creating intermediate lists in memory.',
            answer: '---CODE_START---python\n# Using a generator with recursion for arbitrary nesting depth\ndef flatten(nested_list):\n    for item in nested_list:\n        # If the item is a list, recurse into it\n        if isinstance(item, list):\n            yield from flatten(item)\n        # Otherwise, yield the item itself\n        else:\n            yield item\n\nnested = [1, [2, 3], [4, [5, [6]]], 7]\nflat_list = list(flatten(nested))\nprint("Flattened List:", flat_list)\n# Output: [1, 2, 3, 4, 5, 6, 7]\n---CODE_END---',
            example: '**Code Explanation**:\nThe `flatten` function iterates through the input list. If an `item` is not a list, it `yield`s it, making it part of the output sequence. If the `item` is a list, the `yield from` expression is used. This special syntax tells the generator to delegate to the sub-generator created by the recursive call `flatten(item)`. This effectively "unpacks" the nested list into the main sequence. Finally, `list()` consumes the entire generator iterator to create the final flat list.'
        },
        {
            id: 'pychal-11',
            question: 'Given a log file, extract patterns using regex.',
            concepts: '**Core Concepts**: Regular Expressions (Regex), Text Parsing, Pattern Matching.\n**Explanation**: Regular expressions are a powerful mini-language for finding and extracting patterns from text. They are essential for working with unstructured or semi-structured data like logs, where you need to pull out specific pieces of information like timestamps, IP addresses, or usernames.',
            answer: '---CODE_START---python\nimport re\n\nlog_data = """\nINFO:2023-10-27:User \'alice\' logged in from 192.168.1.10\nERROR:2023-10-27:Failed login for user \'bob\' from 203.0.113.45\n"""\n\n# Define a regex pattern with named capture groups\npattern = re.compile(\n    r"^(?P<level>\\w+):"         # Log level (e.g., INFO)\n    r"(?P<date>[^:]+):"           # Date\n    r".*?user \\\'(?P<user>\\w+)\\\'" # Username\n    r".*?(?P<ip>\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})?" # Optional IP\n)\n\nparsed_logs = []\nfor line in log_data.strip().split(\'\\n\'):\n    match = pattern.search(line)\n    if match:\n        parsed_logs.append(match.groupdict())\n\nprint(parsed_logs)\n# Output: [\n#   {\'level\': \'INFO\', \'date\': \'2023-10-27\', \'user\': \'alice\', \'ip\': \'192.168.1.10\'},\n#   {\'level\': \'ERROR\', \'date\': \'2023-10-27\', \'user\': \'bob\', \'ip\': \'203.0.113.45\'}\n# ]\n---CODE_END---',
            example: '**Code Explanation**:\nThis solution uses `re.compile` for efficiency if the pattern is used many times. The pattern itself uses named capture groups like `(?P<level>\\w+)` which makes extracting the data much easier and more readable. `\\w+` matches one or more word characters. The code iterates through each log line, searches for the pattern, and if a match is found, `match.groupdict()` returns a dictionary where keys are the capture group names (`level`, `date`, etc.) and values are the matched strings.'
        },
        {
            id: 'pychal-12',
            question: 'Sort a DataFrame based on multiple columns.',
            concepts: '**Core Concepts**: Data Sorting, Hierarchical Sorting.\n**Explanation**: Often, you need to sort data by more than one criterion. For example, sorting employees first by department, and then by salary within each department. This is known as a hierarchical or multi-level sort.',
            answer: '---CODE_START---python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \'Department\': [\'Sales\', \'IT\', \'Sales\', \'IT\', \'HR\'],\n    \'Salary\': [70000, 80000, 65000, 95000, 72000],\n    \'HireDate\': [\'2020-03-10\', \'2019-07-15\', \'2021-05-20\', \'2018-01-05\', \'2020-03-10\']\n})\n\n# Sort by Department (ascending) and then by Salary (descending)\nsorted_df = df.sort_values(\n    by=[\'Department\', \'Salary\'], \n    ascending=[True, False]\n)\n\nprint("Sorted DataFrame:\\n", sorted_df)\n---CODE_END---',
            example: '**Code Explanation**:\nThe `df.sort_values()` method is used for sorting. \n- The `by` parameter takes a list of column names, specifying the order of sorting priority. Here, it will sort the entire DataFrame by `Department` first. \n- The `ascending` parameter takes a corresponding list of booleans. `True` means ascending (A-Z, 0-9) and `False` means descending. So, within each department group, it will then sort by `Salary` in descending order (highest to lowest).'
        },
        {
            id: 'pychal-13',
            question: 'Convert categorical columns to numeric without libraries.',
            concepts: '**Core Concepts**: Feature Engineering, Manual Encoding.\n**Explanation**: Before the convenience of libraries like scikit-learn, you had to manually implement encoding. This exercise demonstrates the underlying logic: create a mapping from each unique category to an integer, and then apply that mapping to the entire column.',
            answer: '---CODE_START---python\ndef encode_categorical(column_data):\n    # 1. Find unique categories\n    unique_categories = sorted(list(set(column_data)))\n    \n    # 2. Create a mapping from category to integer\n    mapping = {category: i for i, category in enumerate(unique_categories)}\n    \n    # 3. Apply the mapping to the original data\n    encoded_data = [mapping[item] for item in column_data]\n    \n    return encoded_data, mapping\n\ncategories = [\'red\', \'blue\', \'green\', \'red\', \'blue\', \'blue\']\nencoded, a_mapping = encode_categorical(categories)\n\nprint("Original:", categories)\nprint("Encoded:", encoded)\nprint("Mapping:", a_mapping)\n# Output:\n# Original: [\'red\', \'blue\', \'green\', \'red\', \'blue\', \'blue\']\n# Encoded: [2, 0, 1, 2, 0, 0]\n# Mapping: {\'blue\': 0, \'green\': 1, \'red\': 2}\n---CODE_END---',
            example: '**Code Explanation**:\nThe function follows a three-step process:\n1. It finds all unique categories by converting the list to a `set` and back to a list.\n2. It builds a `mapping` dictionary where each unique category is a key and its integer index (from `enumerate`) is the value.\n3. It uses a list comprehension to iterate through the original data and substitute each category string with its corresponding integer from the mapping.'
        },
        {
            id: 'pychal-14',
            question: 'Given two lists, find their intersection without using set().',
            concepts: '**Core Concepts**: Algorithmic Thinking, Time Complexity.\n**Explanation**: The naive approach is a nested loop (O(n*m) complexity), which is very slow for large lists. A more efficient O(n+m) approach involves using a hash map (a dictionary in Python) to achieve near-instantaneous lookups. This demonstrates the importance of choosing the right data structure for the problem.',
            answer: '---CODE_START---python\ndef find_intersection(list1, list2):\n    # Use the smaller list to create the lookup table for efficiency\n    if len(list1) > len(list2):\n        list1, list2 = list2, list1\n\n    # 1. Create a hash map (dictionary) from the smaller list for O(1) lookups\n    lookup = {item: True for item in list1}\n    \n    # 2. Iterate through the larger list and check for existence in the lookup\n    intersection = []\n    for item in list2:\n        if item in lookup:\n            intersection.append(item)\n            # Optional: remove from lookup to handle duplicates correctly\n            del lookup[item]\n            \n    return intersection\n\nlist_a = [1, 2, 3, 4, 5, 5]\nlist_b = [4, 5, 6, 7, 5]\n\nprint(find_intersection(list_a, list_b))\n# Output: [4, 5, 5]\n---CODE_END---',
            example: '**Code Explanation**:\n1. We first create a `lookup` dictionary from the smaller list. This is a crucial optimization. Storing items as keys gives us average O(1) time complexity for checking if an item exists.\n2. We then iterate through the second (larger) list. For each `item`, we check `if item in lookup`. Because `lookup` is a dictionary, this check is very fast.\n3. If it exists, we add it to our `intersection` list and remove it from the `lookup` to ensure that if duplicates are present in both lists, they are handled correctly.'
        },
        {
            id: 'pychal-15',
            question: 'Implement k-means from scratch using NumPy.',
            concepts: '**Core Concepts**: Unsupervised Learning, Clustering, Centroids, Euclidean Distance, Iterative Algorithms.\n**Explanation**: K-Means is a fundamental clustering algorithm. Implementing it from scratch solidifies understanding of its iterative two-step process: (1) Assign each data point to the nearest cluster center (centroid). (2) Update each centroid to be the mean of the points assigned to it. This process repeats until the centroids no longer move.',
            answer: '---CODE_START---python\nimport numpy as np\n\ndef kmeans(X, n_clusters, max_iters=100):\n    # 1. Randomly initialize centroids from the data points\n    centroids = X[np.random.choice(X.shape[0], n_clusters, replace=False)]\n    \n    for _ in range(max_iters):\n        # 2. Assign clusters (Assignment Step)\n        # Calculate distances from each point to each centroid using broadcasting\n        distances = np.sqrt(((X - centroids[:, np.newaxis])**2).sum(axis=2))\n        labels = np.argmin(distances, axis=0)\n        \n        # 3. Update centroids (Update Step)\n        new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(n_clusters)])\n        \n        # 4. Check for convergence\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n            \n    return labels, centroids\n\n# Example\nX = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\nlabels, centroids = kmeans(X, n_clusters=2)\nprint("Cluster Labels:", labels)\nprint("Final Centroids:\\n", centroids)\n---CODE_END---',
            example: '**Code Explanation**:\nThe core logic is in the `for` loop.\n- **Assignment**: The line calculating `distances` is a powerful piece of NumPy broadcasting. It computes the Euclidean distance between every point in `X` and every `centroid` simultaneously, without a Python loop. `np.argmin` then finds the index of the closest centroid for each point.\n- **Update**: A list comprehension calculates the new centroids by filtering `X` for all points belonging to a cluster (`labels == k`) and taking their `mean`.\n- **Convergence**: The loop breaks early if the centroids don\'t change between iterations.'
        },
        {
            id: 'pychal-16',
            question: 'Write a function to normalize/standardize data.',
            concepts: '**Core Concepts**: Feature Scaling, Normalization (Min-Max), Standardization (Z-score).\n**Explanation**: Many ML algorithms perform poorly if input features are on vastly different scales. \n- **Normalization** scales data to a fixed range, usually [0, 1]. It\'s useful when you need bounded values.\n- **Standardization** transforms data to have a mean of 0 and a standard deviation of 1. It\'s less affected by outliers and is the default choice for many algorithms.',
            answer: '---CODE_START---python\nimport numpy as np\n\ndef scale_data(data, method=\'standardize\'):\n    if method == \'normalize\':\n        min_val = np.min(data, axis=0)\n        max_val = np.max(data, axis=0)\n        return (data - min_val) / (max_val - min_val)\n    elif method == \'standardize\':\n        mean_val = np.mean(data, axis=0)\n        std_val = np.std(data, axis=0)\n        return (data - mean_val) / std_val\n    else:\n        raise ValueError("Method must be \'normalize\' or \'standardize\'")\n\ndata = np.array([[10, 100], [20, 200], [30, 300]], dtype=float)\n\nstandardized = scale_data(data, method=\'standardize\')\nnormalized = scale_data(data, method=\'normalize\')\n\nprint("Standardized (Z-score):\\n", standardized)\nprint("\\nNormalized (Min-Max):\\n", normalized)\n---CODE_END---',
            example: '**Code Explanation**:\nThe function takes a NumPy array `data` and a `method` string. The `axis=0` argument in the NumPy functions is crucial; it ensures the calculations (mean, min, max, std) are performed column-wise, so each feature is scaled independently. The code then applies the respective formula for standardization or normalization to the entire array at once using vectorized operations.'
        },
        {
            id: 'pychal-17',
            question: 'Find the most frequent word in a text file.',
            concepts: '**Core Concepts**: Text Processing, Tokenization, Frequency Analysis.\n**Explanation**: This is a classic NLP (Natural Language Processing) warm-up task. It involves several key steps: reading the text, cleaning it (removing punctuation and converting to a consistent case), breaking it into individual words (tokenization), and finally, counting the frequency of each word to find the most common one.',
            answer: '---CODE_START---python\nimport re\nfrom collections import Counter\n\n# Create a dummy text file\nwith open("sample.txt", "w") as f:\n    f.write("This is a sample sentence. This sentence is for testing.")\n\ndef most_frequent_word(filepath):\n    try:\n        with open(filepath, \'r\') as f:\n            text = f.read()\n    except FileNotFoundError:\n        return "File not found."\n\n    # 1. Clean the text: lowercase and remove punctuation\n    text = text.lower()\n    words = re.findall(r\'\\b\\w+\\b\', text)\n    \n    # 2. Count word frequencies\n    if not words:\n        return "No words found."\n    \n    word_counts = Counter(words)\n    \n    # 3. Find the most frequent word\n    return word_counts.most_common(1)[0]\n\nmost_common = most_frequent_word(\'sample.txt\')\nprint(f"The most frequent word is: \'{most_common[0]}\' with {most_common[1]} occurrences.")\n# Output: The most frequent word is: \'this\' with 2 occurrences.\n---CODE_END---',
            example: '**Code Explanation**:\n1. The function first reads the entire file content into a string.\n2. It converts the text to lowercase to ensure "This" and "this" are treated as the same word. The regex `re.findall(r\'\\b\\w+\\b\', text)` is a robust way to extract all words, ignoring punctuation.\n3. `collections.Counter(words)` creates a dictionary-like object mapping each word to its frequency.\n4. `word_counts.most_common(1)` returns a list containing a single tuple `(\'word\', count)` for the most frequent word.'
        },
        {
            id: 'pychal-18',
            question: 'Parse JSON data and convert it to a DataFrame.',
            concepts: '**Core Concepts**: Data Formats (JSON), Data Ingestion, Handling Semi-structured Data.\n**Explanation**: JSON (JavaScript Object Notation) is a very common format for sending data via APIs. Being able to parse a JSON string and load it into a structured format like a pandas DataFrame is a critical skill for data acquisition.',
            answer: '---CODE_START---python\nimport json\nimport pandas as pd\n\n# Sample JSON string (often comes from an API call)\njson_string = """\n[\n    {"id": 1, "name": "Alice", "city": "New York"},\n    {"id": 2, "name": "Bob", "city": "London"},\n    {"id": 3, "name": "Charlie", "city": "Paris"}\n]\n"""\n\n# 1. Parse the JSON string into a Python object (a list of dictionaries)\npython_data = json.loads(json_string)\n\n# 2. Load the Python object directly into a pandas DataFrame\ndf = pd.DataFrame(python_data)\n\nprint(df)\n---CODE_END---',
            example: '**Code Explanation**:\nThis is a two-step process:\n1. The `json.loads()` function (load string) takes a JSON-formatted string as input and converts it into a corresponding Python data structure. In this case, the JSON array of objects becomes a Python list of dictionaries.\n2. The `pd.DataFrame()` constructor is smart enough to accept a list of dictionaries. It automatically uses the dictionary keys as column headers and the values as the row data, creating the DataFrame in one step.'
        },
        {
            id: 'pychal-19',
            question: 'Given timestamps, compute rolling averages.',
            concepts: '**Core Concepts**: Time Series Analysis, Window Functions, Smoothing.\n**Explanation**: A rolling (or moving) average is a core time series technique used to smooth out short-term fluctuations in data and highlight longer-term trends. It works by creating a sliding window of a fixed size (e.g., 7 days) and calculating the average of the data within that window.',
            answer: '---CODE_START---python\nimport pandas as pd\nimport numpy as np\n\n# Create a sample time series DataFrame\ndates = pd.to_datetime(pd.date_range(\'2023-01-01\', periods=10, freq=\'D\'))\ndata = {\'Sales\': [10, 12, 15, 11, 13, 20, 22, 21, 25, 28]}\ndf = pd.DataFrame(data, index=dates)\n\n# Calculate the 3-day rolling average of Sales\n# The first two values will be NaN because the window is not full yet\ndf[\'Rolling_Avg_3D\'] = df[\'Sales\'].rolling(window=3).mean()\n\nprint(df)\n---CODE_END---',
            example: '**Code Explanation**:\nPandas makes this complex operation very simple with the `.rolling()` method. \n- `df[\'Sales\'].rolling(window=3)` creates a `Rolling` object. It doesn\'t compute anything yet; it just defines a sliding window of size 3 over the `Sales` Series.\n- Chaining `.mean()` to it tells pandas to apply the mean function to each of these 3-period windows. For the date \'2023-01-03\', the window contains [10, 12, 15] and the result is their average, 12.33. For \'2023-01-04\', the window slides to [12, 15, 11], and so on.'
        },
        {
            id: 'pychal-20',
            question: 'Write balanced dataset sampling manually.',
            concepts: '**Core Concepts**: Imbalanced Data, Resampling, Undersampling.\n**Explanation**: When one class in a classification problem is much more frequent than another (e.g., 99% non-fraud vs. 1% fraud), models can become biased towards the majority class. Resampling techniques create a new, balanced dataset for training. Undersampling, shown here, reduces the number of samples from the majority class to match the number in the minority class.',
            answer: '---CODE_START---python\nimport pandas as pd\n\n# Create an imbalanced DataFrame\ndata = {\'feature\': range(20), \'target\': [0]*18 + [1]*2}\ndf = pd.DataFrame(data)\n\n# --- Manual Undersampling Solution ---\ndef balance_dataframe(df, target_col):\n    # 1. Get class counts and identify minority class\n    class_counts = df[target_col].value_counts()\n    minority_class = class_counts.idxmin()\n    minority_size = class_counts.min()\n\n    # 2. Separate majority and minority classes\n    df_minority = df[df[target_col] == minority_class]\n    df_majority = df[df[target_col] != minority_class]\n\n    # 3. Downsample the majority class\n    df_majority_downsampled = df_majority.sample(n=minority_size, random_state=42)\n\n    # 4. Concatenate the balanced dataframes\n    df_balanced = pd.concat([df_majority_downsampled, df_minority])\n\n    return df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n\nbalanced_df = balance_dataframe(df, \'target\')\nprint("Original Counts:\\n", df[\'target\'].value_counts())\nprint("\\nBalanced Counts:\\n", balanced_df[\'target\'].value_counts())\nprint("\\nBalanced DataFrame:\\n", balanced_df)\n---CODE_END---',
            example: '**Code Explanation**:\n1. The function first determines which class is the minority and its size.\n2. It splits the original DataFrame into two: one with only minority class samples and one with only majority class samples.\n3. The key step is `df_majority.sample(n=minority_size)`. This randomly selects a number of rows from the majority DataFrame equal to the size of the minority class.\n4. Finally, it combines this smaller majority sample with the original minority sample using `pd.concat` and shuffles the result to create the final balanced training set.'
        },
        {
            id: 'pychal-21',
            question: 'Generate an Infinite Fibonacci Series Using a Generator.',
            concepts: '**Core Concepts**: Generators, `yield` keyword, Infinite loops, Statefulness.\n**Explanation**: The Fibonacci sequence is a series where each number is the sum of the two preceding ones. A generator is perfect for this because it can compute and `yield` one number at a time, maintaining its state (the last two numbers) between calls. This allows it to produce a potentially infinite sequence without storing all the numbers in memory.',
            answer: '---CODE_START---python\ndef fibonacci_generator():\n    a, b = 0, 1\n    while True:\n        yield a\n        a, b = b, a + b\n\n# --- Usage ---\nfib = fibonacci_generator()\nprint("First 10 Fibonacci numbers:")\nfor _ in range(10):\n    print(next(fib))\n---CODE_END---',
            example: '**Code Explanation**:\nThe function initializes two variables, `a` and `b`, to the first two numbers in the sequence. The `while True:` loop makes the generator potentially infinite. Inside the loop, `yield a` pauses the function and returns the current value of `a`. The next time `next()` is called on the generator, it resumes at the next line, `a, b = b, a + b`, which updates the state to the next two numbers in the sequence, and the loop continues.'
        },
        {
            id: 'pychal-22',
            question: 'Sort a List Without Using the `sort()` method or `sorted()` function.',
            concepts: '**Core Concepts**: Sorting Algorithms, Algorithmic Complexity, In-place modification.\n**Explanation**: This is a classic computer science question to test your understanding of fundamental sorting algorithms. Bubble Sort is one of the simplest to implement. It repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order. This process is repeated until the list is sorted.',
            answer: '---CODE_START---python\ndef bubble_sort(data_list):\n    n = len(data_list)\n    # Traverse through all array elements\n    for i in range(n):\n        # A flag to optimize. If no swaps in an inner loop, list is sorted.\n        swapped = False\n        # Last i elements are already in place\n        for j in range(0, n - i - 1):\n            # Swap if the element found is greater than the next element\n            if data_list[j] > data_list[j + 1]:\n                data_list[j], data_list[j + 1] = data_list[j + 1], data_list[j]\n                swapped = True\n        if not swapped:\n            break\n    return data_list\n\nmy_list = [64, 34, 25, 12, 22, 11, 90]\nprint("Sorted list:", bubble_sort(my_list))\n---CODE_END---',
            example: '**Code Explanation**:\nThe outer loop reduces the number of elements to be checked in the inner loop with each pass, as the largest elements "bubble up" to the end of the list and are already in their correct sorted position. The inner loop performs the actual comparisons and swaps. The `swapped` flag is an optimization that allows the function to exit early if a pass is completed with no swaps, meaning the list is already sorted.'
        },
        {
            id: 'pychal-23',
            question: 'Check Whether a String is a Palindrome or Not.',
            concepts: '**Core Concepts**: String Slicing, String Manipulation, Conditionals.\n**Explanation**: A palindrome is a word, phrase, number, or other sequence of characters that reads the same backward as forward. The most Pythonic way to check this is to compare a string to its reverse. For a robust solution, you should first clean the string by removing spaces and punctuation and converting it to a consistent case.',
            answer: '---CODE_START---python\nimport re\n\ndef is_palindrome(s):\n    # 1. Clean the string: remove non-alphanumeric characters and convert to lowercase\n    cleaned_s = re.sub(r\'[^A-Za-z0-9]+\', \'\', s).lower()\n    \n    # 2. Compare the cleaned string with its reverse\n    return cleaned_s == cleaned_s[::-1]\n\n# Test cases\nprint(f"\'Racecar\': {is_palindrome(\'Racecar\')}")\nprint(f"\'A man, a plan, a canal: Panama\': {is_palindrome(\'A man, a plan, a canal: Panama\')}")\nprint(f"\'hello\': {is_palindrome(\'hello\')}")\n---CODE_END---',
            example: '**Code Explanation**:\nThe regex `re.sub(r\'[^A-Za-z0-9]+\', \'\', s)` finds all characters that are NOT letters or numbers and replaces them with an empty string. The `.lower()` method ensures the comparison is case-insensitive. The core logic is `cleaned_s == cleaned_s[::-1]`. The slice `[::-1]` is a concise Python idiom for reversing a sequence. The function returns the boolean result of this comparison.'
        },
        {
            id: 'pychal-24',
            question: 'Write a Python Program to Find the Factorial of a Number.',
            concepts: '**Core Concepts**: Recursion, Iteration, Base Case.\n**Explanation**: The factorial of a non-negative integer `n`, denoted by `n!`, is the product of all positive integers less than or equal to `n`. For example, `5! = 5 * 4 * 3 * 2 * 1 = 120`. This can be solved iteratively with a loop or recursively, where the function calls itself with a smaller number until it reaches a base case.',
            answer: '---CODE_START---python\n# Iterative approach (more efficient)\ndef factorial_iterative(n):\n    if n < 0:\n        return "Factorial does not exist for negative numbers"\n    elif n == 0:\n        return 1\n    else:\n        result = 1\n        for i in range(1, n + 1):\n            result *= i\n        return result\n\n# Recursive approach\ndef factorial_recursive(n):\n    if n < 0:\n        return "Factorial does not exist for negative numbers"\n    # Base case\n    elif n == 0 or n == 1:\n        return 1\n    # Recursive step\n    else:\n        return n * factorial_recursive(n - 1)\n\nprint("Iterative (5!):", factorial_iterative(5))\nprint("Recursive (5!):", factorial_recursive(5))\n---CODE_END---',
            example: '**Code Explanation**:\n- **Iterative**: It initializes a `result` to 1 and uses a `for` loop to multiply it by each integer from 1 up to `n`.\n- **Recursive**: It defines a base case: if `n` is 0 or 1, it returns 1. Otherwise, it performs the recursive step: it returns `n` multiplied by the result of calling itself with `n-1`. This chain of calls continues until the base case is reached.'
        },
        {
            id: 'pychal-25',
            question: 'Find the Smallest and Largest Number in a List without using `min()` or `max()`.',
            concepts: '**Core Concepts**: Iteration, Comparison, Edge Cases.\n**Explanation**: This fundamental problem tests your ability to handle a list iteratively. The logic involves assuming the first element is both the smallest and largest, then looping through the remaining elements to update these values if a smaller or larger number is found.',
            answer: '---CODE_START---python\ndef find_min_max(data_list):\n    # Handle edge case of an empty list\n    if not data_list:\n        return None, None\n    \n    # Initialize smallest and largest with the first element\n    smallest = data_list[0]\n    largest = data_list[0]\n    \n    # Iterate through the rest of the list\n    for num in data_list[1:]:\n        if num < smallest:\n            smallest = num\n        elif num > largest:\n            largest = num\n            \n    return smallest, largest\n\nmy_list = [64, 34, 25, 12, 22, 11, 90]\ns, l = find_min_max(my_list)\nprint(f"Smallest: {s}, Largest: {l}")\n---CODE_END---',
            example: '**Code Explanation**:\nThe function first checks for an empty list to avoid errors. It then initializes `smallest` and `largest` to the first item. The `for` loop iterates from the *second* item (`data_list[1:]`). In each iteration, it checks if the current `num` is smaller than the current `smallest` or larger than the current `largest` and updates the respective variable if the condition is true.'
        }
    ]
};

export default pythonChallengesCategory;
