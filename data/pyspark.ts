import { QuestionCategory } from '../types';

const pysparkCategory: QuestionCategory = {
    id: 'pyspark',
    title: 'Big Data (PySpark)',
    icon: 'fa-bolt',
    description: "Questions on big data processing with Apache Spark's Python API, focusing on distributed data structures, performance, and execution models.",
    questions: [
        {
            id: 'pyspark-1',
            question: 'What are the differences between RDDs, DataFrames, and Datasets in PySpark?',
            concepts: '**Resilient Distributed Dataset (RDD)**: Spark\'s original, low-level API. An immutable, distributed collection of objects.\n**DataFrame**: A higher-level API built on RDDs that organizes data into a distributed collection of named columns, similar to a relational table.\n**Catalyst Optimizer**: Spark\'s advanced query optimizer that creates efficient execution plans for DataFrame operations.',
            answer: 'This represents the evolution of Spark\'s APIs:\n- **RDD**: The original, low-level abstraction. It is schema-less and offers flexibility but lacks the performance optimizations of newer APIs. You manipulate data with functional transformations like `map()` and `filter()`.\n- **DataFrame**: The most common API in PySpark. It imposes a schema (a structure of named columns) on the data. This structure allows Spark\'s **Catalyst Optimizer** to analyze your code, understand your intent, and create a highly optimized physical execution plan. It is significantly faster than RDDs for most structured data operations.\n- **Dataset**: A typed API, mainly used in Scala/Java. It combines the type-safety of RDDs with the performance benefits of DataFrames. In PySpark, due to Python\'s dynamic nature, DataFrames are the standard and are essentially a collection of `Row` objects (untyped).',
            example: 'Think of an **RDD** as a list of generic Python objects distributed across a cluster. A **DataFrame** is like a distributed pandas DataFrame or a SQL table; you can run optimized queries on it like `df.select("name").where(df.age > 21)`.'
        },
        {
            id: 'pyspark-2',
            question: 'How do Spark’s lazy evaluation and the DAG (Directed Acyclic Graph) execution model work?',
            concepts: '**Lazy Evaluation**: An evaluation strategy that delays the evaluation of an expression until its value is needed.\n**Transformation**: A Spark operation (e.g., `select`, `filter`, `groupBy`) that creates a new DataFrame from an existing one.\n**Action**: A Spark operation (e.g., `count`, `collect`, `save`) that triggers computation and returns a value to the driver or writes to storage.\n**DAG (Directed Acyclic Graph)**: A graph representing the sequence of transformations to be executed.',
            answer: 'Spark\'s efficiency comes from not executing operations immediately.\n1.  **Lazy Evaluation**: When you apply a **transformation** to a DataFrame, Spark does not compute the result right away. Instead, it adds that operation to a logical plan.\n2.  **DAG Creation**: This plan is represented as a **DAG**, where each node is a transformation and the edges represent the data dependency between them.\n3.  **Action Triggers Execution**: The entire DAG is only executed when you call an **action**. At this point, the Catalyst Optimizer analyzes the DAG, creates the most efficient physical execution plan, and runs the job on the cluster.',
            example: 'If you have `df.filter(...).select(...).groupBy(...)`, none of this code runs. Spark just builds a plan (the DAG). Only when you call `.count()` (an action) does Spark figure out the best way to execute the entire sequence, potentially combining filters or pushing them down to the data source to minimize I/O.'
        },
        {
            id: 'pyspark-3',
            question: 'How can you optimize performance in PySpark jobs?',
            concepts: '**Caching/Persistence**: Storing the results of an intermediate DataFrame in memory to avoid recomputation.\n**Partitioning**: Dividing data across the cluster to enable parallelism.\n**Shuffling**: The expensive process of redistributing data across partitions, often required by operations like `groupBy` or joins.',
            answer: 'Several key techniques are crucial for performance:\n- **Caching (`.cache()` or `.persist()`)**: If you use a DataFrame multiple times, cache it in memory to avoid recomputing it from scratch each time.\n- **Minimize Shuffles**: Shuffles are very expensive as they move data across the network. Try to use transformations that avoid them where possible (e.g., use broadcast joins for small tables).\n- **Proper Partitioning**: Ensure your data is well-distributed across partitions to maximize parallelism. Use `repartition()` or `coalesce()` to adjust partition numbers.\n- **Use Built-in Functions**: Avoid Python UDFs (User-Defined Functions) when possible. Spark\'s built-in SQL functions are written in Scala/Java and operate directly within the JVM, avoiding the serialization overhead of UDFs.\n- **Choose the Right File Format**: Use efficient columnar formats like **Parquet** or **ORC**, which support predicate pushdown and compression.',
            example: 'A common pattern is: `df = spark.read.parquet(...)` -> `df_transformed = df.some_transformations()` -> `df_transformed.cache()` -> `df_transformed.count()` -> `df_transformed.write.parquet(...)`. The `.cache()` call prevents `df_transformed` from being recomputed for the `count` and `write` actions.'
        },
        {
            id: 'pyspark-4',
            question: 'How do you handle data skew or imbalanced joins in PySpark?',
            concepts: '**Data Skew**: A condition in which the data is unevenly distributed among partitions in a cluster. This leads to some tasks taking much longer than others, creating a bottleneck.\n**Salting**: A technique to redistribute skewed data by adding a random key to spread it across more partitions.',
            answer: 'Data skew can cripple a Spark job, especially during joins or aggregations where one key has a disproportionately large number of records.\n- **Identify Skew**: You can often detect skew by looking at the Spark UI and seeing a few tasks taking much longer than others for a given stage.\n- **Salting**: This is a common technique. For the skewed key, append a random number (a "salt") from a fixed range (e.g., 1 to 10). Do the same on the other DataFrame by exploding the corresponding key into 10 rows with the same salt range. This effectively splits the skewed key into 10 sub-keys, which Spark can then process in parallel.\n- **Broadcast Joins**: If one of the tables in a join is small enough to fit in memory on each executor, you can broadcast it. This avoids a shuffle entirely and is highly effective against skew.',
            example: 'Imagine joining user activity with user profiles, but one `user_id` (e.g., a guest user) accounts for 50% of the activity. By "salting" this `user_id`, you can break that single massive partition into multiple smaller partitions, balancing the workload across the cluster.'
        },
        {
            id: 'pyspark-5',
            question: 'Explain broadcast variables - what are they and when would you use them?',
            concepts: '**Broadcast Variable**: A mechanism for sharing a read-only variable from the driver to all the executors on the cluster efficiently.\n**Driver vs. Executor**: The driver is the master node that coordinates the Spark job. Executors are worker nodes that perform the actual tasks.',
            answer: 'Normally, when a variable from the driver program is used in a transformation, Spark ships a copy of that variable with every single task. This can be very inefficient if the variable is large.\n\nA **broadcast variable** is a more efficient way to share a large, read-only piece of data (like a lookup dictionary or a small DataFrame) with all worker nodes.\n1. The driver serializes the variable and sends it to each executor **only once**.\n2. The data is then cached on each executor, and all tasks running on that executor can access it without it being re-sent over the network.\n\n**Use Case**: The classic use case is a **broadcast join**, where you join a large DataFrame with a much smaller one. You collect the small DataFrame to the driver, broadcast it, and then use a UDF to perform the join, avoiding a costly shuffle.',
            example: '---CODE_START---python\n# small_lookup_map is a Python dictionary\n# Broadcast it so it\'s available on all executors efficiently\nbroadcast_map = spark.sparkContext.broadcast(small_lookup_map)\n\ndef my_udf(key):\n    # Access the broadcasted value\n    return broadcast_map.value.get(key, "default")\n\n# Now use this UDF in a DataFrame transformation\n# df.withColumn("mapped_value", my_udf(df["key"]))\n---CODE_END---'
        },
        {
            id: 'pyspark-6',
            question: 'Can you walk me through how to implement a PySpark transformation using a custom UDF?',
            concepts: '**UDF (User-Defined Function)**: A function defined by a user that can be used in Spark SQL or DataFrame operations to perform custom logic.\n**Serialization**: The process of converting Python objects into a format that can be transferred over the network to the JVM on the executors.',
            answer: 'While it\'s best to use built-in functions for performance, sometimes you need custom logic that they don\'t provide. A UDF allows you to apply a regular Python function to your DataFrame columns.\n\nHere is the process:\n1.  **Define a Python function**: Write a standard Python function that takes one or more arguments and returns a single value.\n2.  **Create the UDF**: Use `pyspark.sql.functions.udf()` to wrap your Python function. You must specify the return data type (e.g., `StringType()`, `IntegerType()`).\n3.  **Apply the UDF**: Call the created UDF object like a regular Spark function, passing DataFrame columns as arguments.\n\n**Warning**: UDFs have a performance cost. Spark has to serialize the data from the JVM, send it to a Python process, run your function, and then serialize the result back to the JVM. This overhead can be significant. Always try to use built-in functions first.',
            example: '---CODE_START---python\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\n# 1. Define a regular Python function\ndef categorize_length(text):\n    if len(text) < 10:\n        return "short"\n    elif len(text) < 20:\n        return "medium"\n    else:\n        return "long"\n\n# 2. Create the UDF, specifying its return type\ncategorize_udf = udf(categorize_length, StringType())\n\n# Assume \'df\' has a column named "review_text"\n# 3. Apply the UDF to create a new column\ndf_with_category = df.withColumn("text_length_category", categorize_udf(df["review_text"]))\n\ndf_with_category.show()\n---CODE_END---'
        },
        {
            id: 'pyspark-7',
            question: 'Explain Spark’s Catalyst Optimizer. How does it decide the most efficient execution plan?',
            concepts: '**Query Optimization**: The process of choosing the most efficient way to execute a query.\n**Logical Plan vs. Physical Plan**: A logical plan describes what the computation should be, while a physical plan describes how to execute it.',
            answer: 'The Catalyst Optimizer is the core of Spark SQL and DataFrames, and it\'s the main reason for their high performance. It translates your DataFrame/SQL code into a highly optimized execution plan that runs on the cluster.\n\nThe process works in several stages:\n1.  **Analysis**: It first creates an "unresolved logical plan" from your code. It then uses a "catalog" (which contains information about your tables and schemas) to resolve this plan.\n2.  **Logical Optimization**: It applies a set of rule-based optimizations to the logical plan. Examples include **predicate pushdown** (moving `filter` operations closer to the data source to read less data) and **constant folding** (pre-calculating constant expressions).\n3.  **Physical Planning**: Catalyst generates multiple possible physical plans for the optimized logical plan and chooses the one with the lowest estimated cost (based on statistics and cost models).\n4.  **Code Generation**: The final step is to generate efficient Java bytecode that runs on the JVM on each executor. It uses techniques like "whole-stage code generation" to collapse many operations into a single function, reducing virtual function calls and improving CPU/memory usage.',
            example: 'If you write `spark.read.parquet(...).filter(...).select(...)`, the Catalyst Optimizer might rewrite this to first apply the filter directly at the Parquet file level (predicate pushdown), so it only reads the necessary rows, and then it will only read the specific columns requested by `select`, dramatically reducing I/O.'
        },
        {
            id: 'pyspark-8',
            question: 'How do you handle late-arriving data in Spark Structured Streaming using watermarking?',
            concepts: '**Structured Streaming**: Spark\'s high-level API for continuous, scalable stream processing.\n**Event Time**: The time at which an event actually occurred, embedded in the data itself.\n**Watermarking**: A technique to handle late-arriving data in streaming by defining a threshold for how "late" data can be before it is dropped.',
            answer: 'Stateful streaming operations, like windowed aggregations, need to maintain an intermediate state. Without a mechanism to clean up old state, this state would grow indefinitely.\n\n**Watermarking** is how Spark manages this. You define a watermark on an event-time column, which tells the streaming engine: "Don\'t expect any data older than this threshold."\n\nHow it works:\n1.  You specify an event-time column and a delay threshold (e.g., `withWatermark("timestamp", "10 minutes")`).\n2.  As new data arrives, Spark tracks the maximum event time it has seen so far.\n3.  The watermark is calculated as `max_event_time - delay_threshold`.\n4.  Any incoming data with an event time older than the current watermark is considered "too late" and is dropped, allowing Spark to safely drop old state and emit final windowed results.',
            example: '---CODE_START---python\nfrom pyspark.sql.functions import window\n\n# Group data by window and count\nwindowed_counts = events \\\n    .withWatermark("timestamp", "10 minutes") \\\n    .groupBy(window("timestamp", "5 minutes")) \\\n    .count()\n---CODE_END---\nIn this example, Spark will wait up to 10 minutes for late data to arrive for a given 5-minute window. After the watermark passes a window\'s end time, any subsequent data for that window is ignored.'
        },
        {
            id: 'pyspark-9',
            question: 'Explain Slowly Changing Dimensions (SCD) and how you would implement Type 2 logic efficiently in Spark.',
            concepts: '**Slowly Changing Dimension (SCD)**: A dimension that stores and manages both current and historical data over time in a data warehouse.\n**SCD Type 2**: A common method where historical data is preserved by creating new rows for changes. It typically uses effective date ranges (`start_date`, `end_date`) and a current flag (`is_current`).',
            answer: 'SCDs are a data warehousing concept for managing changes in dimension attributes over time. **SCD Type 2** is the most common method for maintaining a full history.\n\nFor example, if a customer moves from California to New York, instead of overwriting their state, you create a new record for New York and expire the old California record.\n\n**Implementing in Spark with Delta Lake is highly efficient**:\n1.  **Identify Changes**: Join your existing dimension table (the target) with the new source data on the natural key (e.g., `customer_id`). Identify new records, updated records, and unchanged records.\n2.  **Use `MERGE INTO`**: Delta Lake\'s `MERGE` command is perfect for this. It can perform inserts, updates, and deletes in a single atomic operation.\n    - `WHEN NOT MATCHED THEN INSERT`: This handles new customers.\n    - `WHEN MATCHED AND source.address <> target.address THEN UPDATE`: This is the core logic. When an existing customer\'s address has changed, you update the `is_current` flag on the old record to false and insert a new record with the updated address and `is_current = true`. This is typically done in two separate merge operations or by using a more complex staging process.',
            example: 'A common pattern is to `FULL OUTER JOIN` the source and target tables. The join result allows you to easily identify three types of rows: (1) new rows present only in the source, (2) expired rows present only in the target (if a user is deleted), and (3) matched rows where you can check for attribute changes. You can then use this intermediate result to drive the `MERGE` operation into the final Delta table.'
        }
    ]
};

export default pysparkCategory;